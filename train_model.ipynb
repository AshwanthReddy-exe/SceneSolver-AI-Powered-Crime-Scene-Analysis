{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-27T10:35:21.834233Z",
     "start_time": "2025-05-27T09:22:05.423908Z"
    }
   },
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# === SAFE GLOBALS FOR PICKLE ===\n",
    "torch.serialization.add_safe_globals([LabelEncoder])\n",
    "\n",
    "# === DATASET LOADER ===\n",
    "def load_dataset(root_dir):\n",
    "    dataset = []\n",
    "    class_dirs = [os.path.join(root_dir, d) for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n",
    "\n",
    "    for class_dir in class_dirs:\n",
    "        class_name = os.path.basename(class_dir)\n",
    "        video_dirs = sorted(glob(os.path.join(class_dir, 'video_*')))\n",
    "\n",
    "        for video_dir in video_dirs:\n",
    "            video_name = os.path.basename(video_dir)\n",
    "            caption_filename = f\"captions_{video_name.split('_')[-1]}.json\"\n",
    "            caption_path = os.path.join(video_dir, caption_filename)\n",
    "\n",
    "            if not os.path.exists(caption_path):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                with open(caption_path, 'r') as f:\n",
    "                    captions = json.load(f)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"‚ùå Skipping malformed JSON: {caption_path}\")\n",
    "                continue\n",
    "\n",
    "            for i in range(64):\n",
    "                frame_key = f\"frame_{i:02d}\"\n",
    "                if frame_key not in captions:\n",
    "                    continue\n",
    "                caption = captions[frame_key]\n",
    "\n",
    "                frame_found = False\n",
    "                for ext in ['jpg', 'png']:\n",
    "                    frame_filename = f\"frame_{i:02d}.{ext}\"\n",
    "                    frame_path = os.path.join(video_dir, frame_filename)\n",
    "                    if os.path.exists(frame_path):\n",
    "                        frame_found = True\n",
    "                        break\n",
    "\n",
    "                if not frame_found:\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    image = Image.open(frame_path).convert(\"RGB\")\n",
    "                    dataset.append({\n",
    "                        \"class\": class_name,\n",
    "                        \"video\": video_name,\n",
    "                        \"frame\": frame_path,\n",
    "                        \"caption\": caption,\n",
    "                        \"image\": image\n",
    "                    })\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# === DATASET CLASS ===\n",
    "class CrimeDataset(Dataset):\n",
    "    def __init__(self, data, processor):\n",
    "        self.data = data\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx][\"image\"]\n",
    "        label = self.data[idx][\"label\"]\n",
    "        inputs = self.processor(images=image, return_tensors=\"pt\")\n",
    "        return {\n",
    "            \"pixel_values\": inputs[\"pixel_values\"].squeeze(0),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# === MODEL ===\n",
    "class CLIPClassifier(nn.Module):\n",
    "    def __init__(self, clip_model, num_classes):\n",
    "        super(CLIPClassifier, self).__init__()\n",
    "        self.clip = clip_model\n",
    "        self.classifier = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        vision_outputs = self.clip.vision_model(pixel_values=pixel_values)\n",
    "        pooled = vision_outputs.pooler_output\n",
    "        embeddings = self.clip.visual_projection(pooled)\n",
    "        return self.classifier(embeddings)\n",
    "\n",
    "# === TRAINING FUNCTION ===\n",
    "def train_clip_classifier(root_dir, epochs=4, batch_size=16, freeze_clip=False):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    raw_data = load_dataset(root_dir)\n",
    "    if len(raw_data) == 0:\n",
    "        raise ValueError(\"‚ùå No valid data found. Please check your dataset path and captions.\")\n",
    "\n",
    "    random.shuffle(raw_data)\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    all_labels = label_encoder.fit_transform([item[\"class\"] for item in raw_data])\n",
    "    for i in range(len(raw_data)):\n",
    "        raw_data[i][\"label\"] = all_labels[i]\n",
    "\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "\n",
    "    clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\", use_fast=True)\n",
    "\n",
    "    if freeze_clip:\n",
    "        for param in clip_model.vision_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    model = CLIPClassifier(clip_model, num_classes)\n",
    "    model = nn.DataParallel(model)  # ‚úÖ Multi-processing ready\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-6)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_data, val_data = train_test_split(raw_data, test_size=0.2, random_state=42)\n",
    "    train_dataset = CrimeDataset(train_data, processor)\n",
    "    val_dataset = CrimeDataset(val_data, processor)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, pin_memory=False)\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss, correct = 0, 0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "            pixel_values = batch[\"pixel_values\"].to(device, non_blocking=True)\n",
    "            labels = batch[\"label\"].to(device, non_blocking=True)\n",
    "\n",
    "            outputs = model(pixel_values)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "        acc = correct / len(train_dataset)\n",
    "        print(f\"‚úÖ Epoch {epoch+1} - Loss: {total_loss:.4f} - Accuracy: {acc:.4f}\")\n",
    "\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                pixel_values = batch[\"pixel_values\"].to(device, non_blocking=True)\n",
    "                labels = batch[\"label\"].to(device, non_blocking=True)\n",
    "\n",
    "                outputs = model(pixel_values)\n",
    "                val_correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "        val_acc = val_correct / len(val_dataset)\n",
    "        print(f\"‚úÖ Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"clip_crime_classifier1.pt\")\n",
    "    torch.save(label_encoder, \"label_encoder.pt\")\n",
    "    print(\"‚úÖ Model and label encoder saved.\")\n",
    "    return model, label_encoder, processor\n",
    "\n",
    "# === INFERENCE FUNCTIONS ===\n",
    "def load_model_for_inference(model_path, label_encoder_path):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    label_encoder = torch.load(label_encoder_path, map_location=device, weights_only=False)\n",
    "    label_names = label_encoder.classes_\n",
    "\n",
    "    clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "    model = CLIPClassifier(clip_model, num_classes=len(label_names))\n",
    "    model = nn.DataParallel(model)  # ‚úÖ MPU-safe\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    return model, processor, label_names, device\n",
    "\n",
    "def predict_image(img_path, model, processor, label_names, device):\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    pixel_values = inputs[\"pixel_values\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values)\n",
    "        probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "        pred_idx = probs.argmax(dim=1).item()\n",
    "        return label_names[pred_idx], probs[0][pred_idx].item()\n",
    "\n",
    "def run_inference(image_path, model_path=\"clip_crime_classifier1.pt\", label_encoder_path=\"label_encoder.pt\"):\n",
    "    model, processor, label_names, device = load_model_for_inference(model_path, label_encoder_path)\n",
    "\n",
    "    if os.path.isfile(image_path):\n",
    "        pred, conf = predict_image(image_path, model, processor, label_names, device)\n",
    "        print(f\"[{image_path}] ‚ûú {pred} ({conf:.2f})\")\n",
    "        return pred, conf\n",
    "\n",
    "    elif os.path.isdir(image_path):\n",
    "        print(f\"\\nüìÅ Inference on folder: {image_path}\")\n",
    "        results = []\n",
    "        for fname in os.listdir(image_path):\n",
    "            if fname.lower().endswith((\".jpg\", \".png\", \".jpeg\")):\n",
    "                fpath = os.path.join(image_path, fname)\n",
    "                pred, conf = predict_image(fpath, model, processor, label_names, device)\n",
    "                print(f\"{fname}: {pred} ({conf:.2f})\")\n",
    "                results.append((fname, pred, conf))\n",
    "        return results\n",
    "\n",
    "    else:\n",
    "        print(f\"‚ùå Invalid path: {image_path}\")\n",
    "        return None\n",
    "\n",
    "# === ENTRY POINT ===\n",
    "if __name__ == \"__main__\":\n",
    "    model, label_encoder, processor = train_clip_classifier(\"/Users/preetham_aleti/Desktop/DATASET\")\n",
    "    run_inference(\"/Users/preetham_aleti/Desktop/DATASET/ROBBERY/video_03/frame_13.jpg\")\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 636/636 [15:47<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 1 - Loss: 104.6688 - Accuracy: 0.9595\n",
      "‚úÖ Validation Accuracy: 0.9992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 636/636 [15:43<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 2 - Loss: 3.3190 - Accuracy: 0.9995\n",
      "‚úÖ Validation Accuracy: 0.9996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 636/636 [16:15<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 3 - Loss: 1.4929 - Accuracy: 0.9998\n",
      "‚úÖ Validation Accuracy: 0.9996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 636/636 [19:44<00:00,  1.86s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 4 - Loss: 0.8564 - Accuracy: 0.9999\n",
      "‚úÖ Validation Accuracy: 1.0000\n",
      "‚úÖ Model and label encoder saved.\n"
     ]
    },
    {
     "ename": "UnpicklingError",
     "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001B[1mdo those steps only if you trust the source of the checkpoint\u001B[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL numpy._core.multiarray._reconstruct was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy._core.multiarray._reconstruct])` or the `torch.serialization.safe_globals([numpy._core.multiarray._reconstruct])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mUnpicklingError\u001B[39m                           Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 235\u001B[39m\n\u001B[32m    233\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[34m__name__\u001B[39m == \u001B[33m\"\u001B[39m\u001B[33m__main__\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    234\u001B[39m     model, label_encoder, processor = train_clip_classifier(\u001B[33m\"\u001B[39m\u001B[33m/Users/preetham_aleti/Desktop/DATASET\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m235\u001B[39m     \u001B[43mrun_inference\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m/Users/preetham_aleti/Desktop/DATASET/ROBBERY/video_03/frame_13.jpg\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 210\u001B[39m, in \u001B[36mrun_inference\u001B[39m\u001B[34m(image_path, model_path, label_encoder_path)\u001B[39m\n\u001B[32m    209\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mrun_inference\u001B[39m(image_path, model_path=\u001B[33m\"\u001B[39m\u001B[33mclip_crime_classifier1.pt\u001B[39m\u001B[33m\"\u001B[39m, label_encoder_path=\u001B[33m\"\u001B[39m\u001B[33mlabel_encoder.pt\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m--> \u001B[39m\u001B[32m210\u001B[39m     model, processor, label_names, device = \u001B[43mload_model_for_inference\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabel_encoder_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    212\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m os.path.isfile(image_path):\n\u001B[32m    213\u001B[39m         pred, conf = predict_image(image_path, model, processor, label_names, device)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 184\u001B[39m, in \u001B[36mload_model_for_inference\u001B[39m\u001B[34m(model_path, label_encoder_path)\u001B[39m\n\u001B[32m    182\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mload_model_for_inference\u001B[39m(model_path, label_encoder_path):\n\u001B[32m    183\u001B[39m     device = torch.device(\u001B[33m\"\u001B[39m\u001B[33mcuda\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch.cuda.is_available() \u001B[38;5;28;01melse\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mcpu\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m184\u001B[39m     label_encoder = \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlabel_encoder_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmap_location\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    185\u001B[39m     label_names = label_encoder.classes_\n\u001B[32m    187\u001B[39m     clip_model = CLIPModel.from_pretrained(\u001B[33m\"\u001B[39m\u001B[33mopenai/clip-vit-base-patch32\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/scenesolver/.venv/lib/python3.13/site-packages/torch/serialization.py:1524\u001B[39m, in \u001B[36mload\u001B[39m\u001B[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001B[39m\n\u001B[32m   1516\u001B[39m                 \u001B[38;5;28;01mreturn\u001B[39;00m _load(\n\u001B[32m   1517\u001B[39m                     opened_zipfile,\n\u001B[32m   1518\u001B[39m                     map_location,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1521\u001B[39m                     **pickle_load_args,\n\u001B[32m   1522\u001B[39m                 )\n\u001B[32m   1523\u001B[39m             \u001B[38;5;28;01mexcept\u001B[39;00m pickle.UnpicklingError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m-> \u001B[39m\u001B[32m1524\u001B[39m                 \u001B[38;5;28;01mraise\u001B[39;00m pickle.UnpicklingError(_get_wo_message(\u001B[38;5;28mstr\u001B[39m(e))) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1525\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m _load(\n\u001B[32m   1526\u001B[39m             opened_zipfile,\n\u001B[32m   1527\u001B[39m             map_location,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1530\u001B[39m             **pickle_load_args,\n\u001B[32m   1531\u001B[39m         )\n\u001B[32m   1532\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m mmap:\n",
      "\u001B[31mUnpicklingError\u001B[39m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001B[1mdo those steps only if you trust the source of the checkpoint\u001B[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL numpy._core.multiarray._reconstruct was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy._core.multiarray._reconstruct])` or the `torch.serialization.safe_globals([numpy._core.multiarray._reconstruct])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T10:39:23.623832Z",
     "start_time": "2025-05-27T10:39:13.838694Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_model_for_inference(model_path, label_encoder_path):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    label_encoder = torch.load(label_encoder_path, map_location=device, weights_only=False)\n",
    "    label_names = label_encoder.classes_\n",
    "\n",
    "    clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "    model = CLIPClassifier(clip_model, num_classes=len(label_names))\n",
    "    model = nn.DataParallel(model)  # ‚úÖ MPU-safe\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    return model, processor, label_names, device\n",
    "run_inference(\"/Users/preetham_aleti/Desktop/DATASET/ROBBERY/video_03/frame_13.jpg\")"
   ],
   "id": "e8a5c40c34bdb022",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[/Users/preetham_aleti/Desktop/DATASET/ROBBERY/video_03/frame_13.jpg] ‚ûú ROBBERY (1.00)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.str_('ROBBERY'), 0.9993220567703247)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "\n",
    "def load_clip_model(model_path):\n",
    "    \"\"\"\n",
    "    Load CLIP model with advanced weight loading strategies.\n",
    "\n",
    "    Args:\n",
    "        model_path (str): Path to the model checkpoint file\n",
    "\n",
    "    Returns:\n",
    "        tuple: Loaded model and processor\n",
    "    \"\"\"\n",
    "    # Initialize base CLIP model\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "    # Ensure the model path exists\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model checkpoint not found at {model_path}\")\n",
    "\n",
    "    try:\n",
    "        # Load state dict\n",
    "        state_dict = torch.load(model_path, map_location=\"cpu\")\n",
    "\n",
    "        # Print out available keys for debugging\n",
    "        print(\"Available keys in state dict:\")\n",
    "        print(\"\\n\".join(state_dict.keys()))\n",
    "\n",
    "        # Advanced weight loading strategies\n",
    "        try:\n",
    "            # Strategy 1: Try loading as-is\n",
    "            model.load_state_dict(state_dict, strict=False)\n",
    "            print(\"‚úÖ Loaded model weights successfully with loose matching.\")\n",
    "        except Exception as full_load_error:\n",
    "            print(f\"Full model load failed: {full_load_error}\")\n",
    "\n",
    "            # Strategy 2: Try loading vision model weights specifically\n",
    "            try:\n",
    "                # Extract vision model weights\n",
    "                vision_dict = {\n",
    "                    k.replace(\"vision_model.\", \"\"): v\n",
    "                    for k, v in state_dict.items()\n",
    "                    if k.startswith(\"vision_model.\")\n",
    "                }\n",
    "\n",
    "                # If no vision model keys found, try matching partial keys\n",
    "                if not vision_dict:\n",
    "                    vision_dict = {\n",
    "                        k.split(\".\")[-1]: v\n",
    "                        for k, v in state_dict.items()\n",
    "                        if any(part in k for part in ['vision', 'embed', 'layer', 'norm'])\n",
    "                    }\n",
    "\n",
    "                # Partial loading with custom matching\n",
    "                missing_keys = []\n",
    "                unexpected_keys = []\n",
    "                model.vision_model.load_state_dict(\n",
    "                    vision_dict,\n",
    "                    strict=False,\n",
    "                    missing_keys=missing_keys,\n",
    "                    unexpected_keys=unexpected_keys\n",
    "                )\n",
    "\n",
    "                print(\"‚ö†Ô∏è Loaded vision model weights with partial matching.\")\n",
    "                if missing_keys:\n",
    "                    print(\"Missing keys:\", missing_keys)\n",
    "                if unexpected_keys:\n",
    "                    print(\"Unexpected keys:\", unexpected_keys)\n",
    "\n",
    "            except Exception as vision_load_error:\n",
    "                print(f\"Vision model load failed: {vision_load_error}\")\n",
    "                raise RuntimeError(\"Unable to load model weights\") from vision_load_error\n",
    "\n",
    "        model.eval()\n",
    "    except Exception as e:\n",
    "        print(f\"Critical error loading model weights: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Initialize the processor for CLIP\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "    return model, processor\n",
    "\n",
    "def classify_image(model, processor, image_path, text_labels, top_k=4):\n",
    "    \"\"\"\n",
    "    Classify an image using CLIP model with given text labels.\n",
    "\n",
    "    Args:\n",
    "        model (CLIPModel): Loaded CLIP model\n",
    "        processor (CLIPProcessor): CLIP processor\n",
    "        image_path (str): Path to the input image\n",
    "        text_labels (list): List of text labels to compare against\n",
    "        top_k (int, optional): Number of top predictions to return. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        list: Top K predictions with labels and scores\n",
    "    \"\"\"\n",
    "    # Validate inputs\n",
    "    if not os.path.exists(image_path):\n",
    "        raise FileNotFoundError(f\"Image not found at {image_path}\")\n",
    "\n",
    "    # Load and preprocess image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    # Encode inputs\n",
    "    inputs = processor(text=text_labels, images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Normalize embeddings & compute similarity\n",
    "    image_embeds = outputs.image_embeds / outputs.image_embeds.norm(dim=-1, keepdim=True)\n",
    "    text_embeds = outputs.text_embeds / outputs.text_embeds.norm(dim=-1, keepdim=True)\n",
    "    similarities = (image_embeds @ text_embeds.T)[0]  # shape: [num_text_labels]\n",
    "\n",
    "    # Get top-K predictions\n",
    "    top_k_indices = similarities.topk(top_k).indices\n",
    "    top_k_labels = [text_labels[i] for i in top_k_indices]\n",
    "    top_k_scores = [similarities[i].item() for i in top_k_indices]\n",
    "\n",
    "    return list(zip(top_k_labels, top_k_scores))\n",
    "\n",
    "def main():\n",
    "    # Configuration\n",
    "    model_path = \"clip_crime_classifier.pt\"  # Path to fine-tuned model weights\n",
    "    image_path = \"/Users/preetham_aleti/Desktop/istockphoto-955124060-612x612.jpg\"# Replace with actual image path\n",
    "\n",
    "    # Text labels (same as original script)\n",
    "    text_labels = [\n",
    "    \"knife\", \"gun\", \"blood\", \"weapon\", \"dead\", \"lying\", \"suspicious\", \"fight\", \"attack\", \"injury\", \"explosion\", \"fire\",\n",
    "    \"pistol\", \"rifle\", \"body\", \"corpse\", \"skull\", \"molotov\", \"bullet shells\", \"syringe\", \"chains\", \"rope\", \"handcuffs\",\n",
    "    \"bat\", \"metal rod\", \"glass shard\", \"drugs\", \"cash bundle\", \"suitcase\", \"torn clothes\", \"broken phone\", \"firearm\",\n",
    "    \"blunt object\", \"axe\", \"machete\", \"hammer\", \"crowbar\", \"scalpel\", \"shiv\", \"box cutter\", \"brass knuckles\",\n",
    "    \"stun gun\", \"pepper spray\", \"garrote\", \"baton\", \"metal pipe\", \"switchblade\", \"nail gun\", \"bullet\", \"shell casing\",\n",
    "    \"spent cartridge\", \"gunpowder residue\", \"blood spatter\", \"brain matter\", \"hair strands\", \"fingerprint\", \"shoeprint\",\n",
    "    \"footprint\", \"broken glass\", \"semen stain\", \"urine sample\", \"burn marks\", \"ligature marks\", \"defensive wounds\",\n",
    "    \"duct tape\", \"zip tie\", \"gag\", \"blindfold\", \"chloroform bottle\", \"acid bottle\", \"gloves\", \"balaclava\", \"ski mask\",\n",
    "    \"face mask\", \"surgical gloves\", \"latex gloves\", \"bloody cloth\", \"wet towel\", \"gasoline can\", \"fire debris\",\n",
    "    \"ash residue\", \"charred remains\", \"scorch mark\", \"bullet hole\", \"burnt paper\", \"exploded device\", \"broken lock\",\n",
    "    \"bent door\", \"scratched surface\", \"lockpick set\", \"ID card\", \"fake passport\", \"credit card\", \"stolen phone\",\n",
    "    \"burner phone\", \"sim card\", \"CCTV footage\", \"notebook\", \"map with markings\", \"surveillance photo\", \"threat letter\",\n",
    "    \"ransom note\", \"USB drive\", \"hard disk\", \"memory card\", \"wallet\", \"driver's license\", \"personal photo\", \"watch\",\n",
    "    \"ring\", \"bloodied shoe\", \"muddy boots\", \"missing clothing\", \"ripped shirt\", \"torn dress\", \"evidence bag\",\n",
    "    \"crime scene tape\", \"silencer\", \"tripwire\", \"booby trap\", \"grenade\", \"sniper rifle\", \"projectile\", \"smoke bomb\",\n",
    "    \"carbon monoxide canister\", \"fire starter\", \"fuse wire\", \"circuit board\", \"detonator\", \"gas leak\", \"blood trail\",\n",
    "    \"drag marks\", \"vomit stain\", \"fingerprint powder\", \"DNA swab\", \"bite mark\", \"nail scrapings\", \"glass particles\",\n",
    "    \"paint chip\", \"soil sample\", \"fibers\", \"gunshot residue\", \"blood droplets\", \"taser\", \"wire cutters\", \"pliers\",\n",
    "    \"torch\", \"headlamp\", \"ladder\", \"paracord\", \"spiked object\", \"whip\", \"screwdriver\", \"socket wrench\", \"drill\",\n",
    "    \"nail\", \"saw\", \"shovel\", \"climbing gear\", \"ski cap\", \"work boots\", \"combat boots\", \"disguise\", \"makeup kit\",\n",
    "    \"fake beard\", \"wig\", \"mirror fragment\", \"red-stained clothing\", \"soaked glove\", \"burnt shoes\", \"burnt wallet\",\n",
    "    \"twine\", \"twisted metal\", \"credit card reader\", \"POS skimmer\", \"pin pad\", \"cloned card\", \"forged signature\",\n",
    "    \"bank note\", \"fake cheque\", \"contract\", \"confession note\", \"manifest\", \"registry\", \"diary\", \"blackmail note\",\n",
    "    \"spy camera\", \"drone\", \"bug detector\", \"tracking device\", \"microphone\", \"hidden recorder\", \"wiretap\",\n",
    "    \"GPS tracker\", \"browser history\", \"deleted messages\", \"burned documents\", \"printer ink\", \"USB cable\",\n",
    "    \"data cable\", \"footprint in blood\", \"smeared prints\", \"smashed camera\", \"punch hole\", \"broken furniture\",\n",
    "    \"spray paint\", \"graffiti\", \"blood pool\", \"trail of tears\", \"hysterical victim\", \"witness statement\",\n",
    "    \"torn notebook\", \"security tag\", \"alarm sensor\", \"motion detector\", \"entry log\", \"access card\", \"forged badge\",\n",
    "    \"suspicious note\", \"powdery substance\", \"chemical residue\", \"gas mask\", \"hazmat suit\", \"burner laptop\",\n",
    "    \"encrypted phone\", \"person running\", \"flames fire\",\"Explosion aftermath\",\"Rescue operation\",\"Disaster response\",\"Fire damage\",\n",
    "        \"Explosion aftermath\", \"Fire damage\", \"Scattered debris\", \"Black smoke\", \"Firefighters at scene\",\n",
    "\"Destroyed vehicles\", \"Collapsed buildings\", \"Rescue operation\", \"Emergency response units\",\n",
    "\"Injured civilians\", \"Charred remains\", \"Flash burns\", \"Smoke inhalation victims\", \"Craters on ground\",\n",
    "\"Damaged infrastructure\", \"Shattered glass\", \"Shockwave damage\", \"Burnt clothing\", \"Heat distortion\",\n",
    "\"Bomb squad presence\", \"Evacuation in progress\", \"Military cordon\", \"Hazmat suits\", \"Surveillance footage of blast\",\n",
    "\"Sound of explosion\", \"Witness reports\", \"Forensic chemical traces\", \"Explosive residue\", \"Satellite imagery of blast\",\n",
    "\"Thermal imaging\", \"Before/after photos\", \"Seismograph spike\", \"Damaged electronics\", \"Overturned vehicles\",\n",
    "\"Melted metals\", \"High-temperature indicators\", \"Blast pressure marks\", \"Security camera destruction\",\n",
    "\"Media reporting live\", \"Flash of light\", \"Sudden power outages\", \"Flying debris injuries\",\n",
    "\"Hospital surge\", \"Burn units activated\", \"Soot-covered victims\", \"Displaced people\", \"Airspace restrictions\",\n",
    "\"Shockwave-injured animals\", \"Search and rescue dogs\", \"Structural engineers onsite\",\n",
    "\n",
    "\"Physical altercation\", \"Punches thrown\", \"Visible bruises\", \"Blood stains\", \"Crowd gathering\",\n",
    "\"Police breaking up fight\", \"Verbal aggression\", \"Weapons drawn\", \"Security camera footage\", \"Bodycam recordings\",\n",
    "\"CCTV footage\", \"Broken bottles\", \"Pulled hair\", \"Torn clothing\", \"Screaming\", \"Knife wounds\",\n",
    "\"Hospital reports\", \"Eyewitness testimony\", \"Self-defense stances\", \"Bystander videos\", \"Arrest records\",\n",
    "\"911 calls\", \"Social media footage\", \"Conflicting statements\", \"Injury reports\", \"Physical evidence on ground\",\n",
    "\"Trampled area\", \"Thrown objects\", \"Police batons\", \"Tasers used\", \"Pepper spray evidence\", \"Handcuff marks\",\n",
    "\"Footage of chase\", \"Disturbed public area\", \"Fight started in queue\", \"Nightclub security footage\",\n",
    "\"Bar fight scene\", \"Street brawl\", \"Schoolyard altercation\", \"Confiscated weapons\", \"Face injuries\",\n",
    "\"Split lips\", \"Black eyes\", \"Smashed furniture\", \"Police statement\", \"Witness cell footage\", \"Brawl aftermath\",\n",
    "\"Adrenaline effects\", \"Medical treatment logs\", \"Offensive gestures\",\n",
    "\n",
    "\"Forced entry\", \"Broken locks\", \"Surveillance footage\", \"Mask-wearing suspect\", \"Glove prints\",\n",
    "\"Empty safe\", \"Stolen valuables\", \"Threats with weapon\", \"Demand notes\", \"Security alarm triggered\",\n",
    "\"Witness testimony\", \"Gunshot sound\", \"Panic button pressed\", \"Store clerks hiding\", \"Cash register emptied\",\n",
    "\"Abandoned getaway vehicle\", \"Dropped loot\", \"Forensic evidence\", \"Glove fibers\", \"Security tape review\",\n",
    "\"Bank teller account\", \"Vault tampering\", \"ATM ripped open\", \"Smashed glass\", \"Crowbar found\",\n",
    "\"CCTV malfunction\", \"Phone jammers\", \"Multiple suspects\", \"Police pursuit\", \"Police sketches\",\n",
    "\"Security guard overpowered\", \"Use of disguise\", \"Timing of heist\", \"Inside job suspicion\",\n",
    "\"Drilled locks\", \"Camera blackout\", \"Customer witness\", \"Fake IDs\", \"Unusual deposits\", \"Tool marks\",\n",
    "\"Fingerprint recovery\", \"Silent alarm logs\", \"DNA swab from scene\", \"Safe-cracking tools\", \"Entry point photos\",\n",
    "\"Surrounding business footage\", \"Abandoned evidence\", \"Loot bag found\", \"Fake license plates\",\n",
    "\"Escape route tracing\", \"Police barrier breach\",\n",
    "\n",
    "\"Unpaid items in bag\", \"Security footage\", \"No receipt\", \"Concealed merchandise\", \"Price tag removal\",\n",
    "\"Security tag tampering\", \"In-store alarms\", \"Store detective report\", \"Suspicious behavior\", \"Camera footage\",\n",
    "\"Bag check failure\", \"Hidden compartments\", \"Items under clothing\", \"Unusual customer route\", \"Acting nervously\",\n",
    "\"Rushed exit\", \"Cashier distraction\", \"Blind spots visited\", \"Multiple entry attempts\", \"Dressing room concealment\",\n",
    "\"Magnetic detacher possession\", \"Recovered stolen goods\", \"Admission of guilt\", \"Inventory mismatch\",\n",
    "\"Staff report\", \"Frequent offender\", \"Cellphone jammer\", \"Fake returns\", \"Returned stolen items\",\n",
    "\"Store layout exploitation\", \"Distraction technique\", \"Witness confrontation\", \"Unattended carts\",\n",
    "\"Companion as lookout\", \"Fake disability use\", \"Suspicious bag size\", \"Overcrowded checkout lane\",\n",
    "\"Tampered packaging\", \"Evasive responses\", \"Rapid clothing change\", \"Discarded wrappers\", \"Matching descriptions\",\n",
    "\"Unscanned items\", \"Detained by staff\", \"Portable scanner blocker\", \"Frequent item handling\",\n",
    "\"Retail chain alert\", \"Repeat pattern detection\", \"Reluctance to open bag\", \"Disguised identity\"\n",
    "]\n",
    "\n",
    "\n",
    "    try:\n",
    "        # Load model and processor\n",
    "        model, processor = load_clip_model(model_path)\n",
    "\n",
    "        # Classify image\n",
    "        predictions = classify_image(model, processor, image_path, text_labels)\n",
    "\n",
    "        # Display results\n",
    "        print(\"\\n--- Top Predictions ---\")\n",
    "        for rank, (label, score) in enumerate(predictions, 1):\n",
    "            print(f\"Top {rank}: {label} (Score: {score:.3f})\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "559aca4507910497"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T11:02:29.712024Z",
     "start_time": "2025-05-27T10:40:55.252700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel, AutoTokenizer, BartForConditionalGeneration\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "from typing import List, Tuple, Dict\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class CLIPVideoInference:\n",
    "    def __init__(self, model_path: str = \"clip_crime_classifier.pt\", device: str = None):\n",
    "        if device is None:\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        else:\n",
    "            self.device = torch.device(device)\n",
    "\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        if self.device.type == \"cuda\":\n",
    "            print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "        print(f\"Loading model from: {model_path}\")\n",
    "        try:\n",
    "            self.model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "            self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "            checkpoint = torch.load(model_path, map_location=self.device)\n",
    "            if 'model_state_dict' in checkpoint:\n",
    "                self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            elif 'state_dict' in checkpoint:\n",
    "                self.model.load_state_dict(checkpoint['state_dict'])\n",
    "            else:\n",
    "                self.model.load_state_dict(checkpoint)\n",
    "\n",
    "            self.model.to(self.device)\n",
    "            self.model.eval()\n",
    "            print(\"‚úÖ Custom CLIP model loaded successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to load model: {e}\")\n",
    "\n",
    "    def load_captions_from_file(self, file_path: str) -> List[str]:\n",
    "        with open(file_path, 'r') as f:\n",
    "            captions = [line.strip() for line in f.readlines() if line.strip()]\n",
    "        print(f\"Loaded {len(captions)} captions\")\n",
    "        return captions\n",
    "\n",
    "    def extract_frames_with_timestamps(self, video_path: str, num_frames: int = 80) -> Tuple[List[np.ndarray], List[float]]:\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            raise ValueError(f\"Cannot open video: {video_path}\")\n",
    "\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        duration = total_frames / fps\n",
    "\n",
    "        indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
    "        frames, timestamps = [], []\n",
    "\n",
    "        for idx in indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frames.append(rgb)\n",
    "                timestamps.append(idx / fps)\n",
    "\n",
    "        cap.release()\n",
    "        print(f\"Extracted {len(frames)} frames from {video_path}\")\n",
    "        return frames, timestamps\n",
    "\n",
    "    def encode_frame(self, frame: np.ndarray) -> torch.Tensor:\n",
    "        pil_image = Image.fromarray(frame)\n",
    "        inputs = self.processor(images=[pil_image], return_tensors=\"pt\").to(self.device)\n",
    "        with torch.no_grad():\n",
    "            features = self.model.get_image_features(**inputs)\n",
    "            features = features / features.norm(dim=-1, keepdim=True)\n",
    "        return features\n",
    "\n",
    "    def encode_texts(self, captions: List[str]) -> torch.Tensor:\n",
    "        all_feats = []\n",
    "        for i in range(0, len(captions), 32):\n",
    "            batch = captions[i:i+32]\n",
    "            inputs = self.processor(text=batch, return_tensors=\"pt\", padding=True, truncation=True).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                feats = self.model.get_text_features(**inputs)\n",
    "                feats = feats / feats.norm(dim=-1, keepdim=True)\n",
    "                all_feats.append(feats)\n",
    "        return torch.cat(all_feats)\n",
    "\n",
    "    def get_best_caption(self, frames: List[np.ndarray], captions: List[str]) -> Tuple[str, float]:\n",
    "        frame_feats = [self.encode_frame(f) for f in frames]\n",
    "        avg_feat = torch.mean(torch.stack(frame_feats), dim=0)\n",
    "        text_feats = self.encode_texts(captions)\n",
    "        sim = torch.matmul(avg_feat, text_feats.T).squeeze()\n",
    "        best_idx = torch.argmax(sim).item()\n",
    "        return captions[best_idx], sim[best_idx].item()\n",
    "\n",
    "    def generate_video_story(self, video_path: str, captions: List[str], segments: int = 20, frames_per_segment: int = 4) -> Dict:\n",
    "        total_frames = segments * frames_per_segment\n",
    "        frames, timestamps = self.extract_frames_with_timestamps(video_path, total_frames)\n",
    "\n",
    "        if len(frames) < total_frames:\n",
    "            print(\"‚ö†Ô∏è Not enough frames, reducing number of segments\")\n",
    "            segments = len(frames) // frames_per_segment\n",
    "\n",
    "        used = set()\n",
    "        story = []\n",
    "\n",
    "        for i in range(segments):\n",
    "            start = i * frames_per_segment\n",
    "            end = start + frames_per_segment\n",
    "            seg_frames = frames[start:end]\n",
    "            seg_start, seg_end = timestamps[start], timestamps[end-1]\n",
    "\n",
    "            available = [cap for cap in captions if cap not in used] or captions\n",
    "            caption, score = self.get_best_caption(seg_frames, available)\n",
    "            used.add(caption)\n",
    "\n",
    "            story.append({\n",
    "                \"sequence\": i+1,\n",
    "                \"time_range\": f\"{seg_start:.1f}s - {seg_end:.1f}s\",\n",
    "                \"caption\": caption,\n",
    "                \"confidence\": round(score, 4)\n",
    "            })\n",
    "\n",
    "            print(f\"[{i+1}] {caption} ({score:.3f})\")\n",
    "\n",
    "        return {\n",
    "            \"video_path\": video_path,\n",
    "            \"segments\": story,\n",
    "            \"duration\": timestamps[-1] if timestamps else 0\n",
    "        }\n",
    "\n",
    "    def display_story(self, result: Dict):\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"üé¨ VIDEO STORY: {os.path.basename(result['video_path'])}\")\n",
    "        print(\"=\"*80)\n",
    "        for item in result[\"segments\"]:\n",
    "            print(f\"üïí {item['time_range']} | üí¨ {item['caption']} (confidence: {item['confidence']:.3f})\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "    def get_captions_list(self, result: Dict) -> List[str]:\n",
    "        return [item['caption'] for item in result['segments']]\n",
    "\n",
    "\n",
    "def summarize_captions(captions: List[str], device: torch.device = torch.device(\"cpu\")) -> str:\n",
    "    print(\"\\nüìö Summarizing captions using BART...\")\n",
    "    model_name = \"facebook/bart-large-cnn\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = BartForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "    text_input = \" \".join(captions)\n",
    "    inputs = tokenizer([text_input], max_length=1024, return_tensors='pt', truncation=True).to(device)\n",
    "\n",
    "    summary_ids = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=300,\n",
    "        min_length=100,\n",
    "        length_penalty=2.0,\n",
    "        num_beams=6,\n",
    "        no_repeat_ngram_size=4,\n",
    "        early_stopping=True,\n",
    "        repetition_penalty=1.5,\n",
    "        temperature=0.9,\n",
    "        top_k=50,\n",
    "        top_p=0.95\n",
    "    )\n",
    "\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary.strip()\n",
    "\n",
    "\n",
    "# === Main Script ===\n",
    "if __name__ == \"__main__\":\n",
    "    video_path = \"/Users/preetham_aleti/Downloads/Explosion/Explosion050_x264A/Explosion050_x264A.mp4\"\n",
    "    captions_file = \"/Users/preetham_aleti/Desktop/captions.txt\"\n",
    "    model_path = \"clip_crime_classifier1.pt\"\n",
    "\n",
    "    # Inference\n",
    "    inferencer = CLIPVideoInference(model_path=model_path)\n",
    "    available_captions = inferencer.load_captions_from_file(captions_file)\n",
    "    result = inferencer.generate_video_story(video_path, available_captions)\n",
    "    inferencer.display_story(result)\n",
    "\n",
    "    # Summarization\n",
    "    segment_captions = inferencer.get_captions_list(result)\n",
    "    summary = summarize_captions(segment_captions)\n",
    "    print(\"\\nüìå Final Video Summary:\\n\" + summary)\n"
   ],
   "id": "8f243f43ddaf5bc3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading model from: clip_crime_classifier1.pt\n",
      "‚ùå Failed to load model: Error(s) in loading state_dict for CLIPModel:\n",
      "\tMissing key(s) in state_dict: \"logit_scale\", \"text_model.embeddings.token_embedding.weight\", \"text_model.embeddings.position_embedding.weight\", \"text_model.encoder.layers.0.self_attn.k_proj.weight\", \"text_model.encoder.layers.0.self_attn.k_proj.bias\", \"text_model.encoder.layers.0.self_attn.v_proj.weight\", \"text_model.encoder.layers.0.self_attn.v_proj.bias\", \"text_model.encoder.layers.0.self_attn.q_proj.weight\", \"text_model.encoder.layers.0.self_attn.q_proj.bias\", \"text_model.encoder.layers.0.self_attn.out_proj.weight\", \"text_model.encoder.layers.0.self_attn.out_proj.bias\", \"text_model.encoder.layers.0.layer_norm1.weight\", \"text_model.encoder.layers.0.layer_norm1.bias\", \"text_model.encoder.layers.0.mlp.fc1.weight\", \"text_model.encoder.layers.0.mlp.fc1.bias\", \"text_model.encoder.layers.0.mlp.fc2.weight\", \"text_model.encoder.layers.0.mlp.fc2.bias\", \"text_model.encoder.layers.0.layer_norm2.weight\", \"text_model.encoder.layers.0.layer_norm2.bias\", \"text_model.encoder.layers.1.self_attn.k_proj.weight\", \"text_model.encoder.layers.1.self_attn.k_proj.bias\", \"text_model.encoder.layers.1.self_attn.v_proj.weight\", \"text_model.encoder.layers.1.self_attn.v_proj.bias\", \"text_model.encoder.layers.1.self_attn.q_proj.weight\", \"text_model.encoder.layers.1.self_attn.q_proj.bias\", \"text_model.encoder.layers.1.self_attn.out_proj.weight\", \"text_model.encoder.layers.1.self_attn.out_proj.bias\", \"text_model.encoder.layers.1.layer_norm1.weight\", \"text_model.encoder.layers.1.layer_norm1.bias\", \"text_model.encoder.layers.1.mlp.fc1.weight\", \"text_model.encoder.layers.1.mlp.fc1.bias\", \"text_model.encoder.layers.1.mlp.fc2.weight\", \"text_model.encoder.layers.1.mlp.fc2.bias\", \"text_model.encoder.layers.1.layer_norm2.weight\", \"text_model.encoder.layers.1.layer_norm2.bias\", \"text_model.encoder.layers.2.self_attn.k_proj.weight\", \"text_model.encoder.layers.2.self_attn.k_proj.bias\", \"text_model.encoder.layers.2.self_attn.v_proj.weight\", \"text_model.encoder.layers.2.self_attn.v_proj.bias\", \"text_model.encoder.layers.2.self_attn.q_proj.weight\", \"text_model.encoder.layers.2.self_attn.q_proj.bias\", \"text_model.encoder.layers.2.self_attn.out_proj.weight\", \"text_model.encoder.layers.2.self_attn.out_proj.bias\", \"text_model.encoder.layers.2.layer_norm1.weight\", \"text_model.encoder.layers.2.layer_norm1.bias\", \"text_model.encoder.layers.2.mlp.fc1.weight\", \"text_model.encoder.layers.2.mlp.fc1.bias\", \"text_model.encoder.layers.2.mlp.fc2.weight\", \"text_model.encoder.layers.2.mlp.fc2.bias\", \"text_model.encoder.layers.2.layer_norm2.weight\", \"text_model.encoder.layers.2.layer_norm2.bias\", \"text_model.encoder.layers.3.self_attn.k_proj.weight\", \"text_model.encoder.layers.3.self_attn.k_proj.bias\", \"text_model.encoder.layers.3.self_attn.v_proj.weight\", \"text_model.encoder.layers.3.self_attn.v_proj.bias\", \"text_model.encoder.layers.3.self_attn.q_proj.weight\", \"text_model.encoder.layers.3.self_attn.q_proj.bias\", \"text_model.encoder.layers.3.self_attn.out_proj.weight\", \"text_model.encoder.layers.3.self_attn.out_proj.bias\", \"text_model.encoder.layers.3.layer_norm1.weight\", \"text_model.encoder.layers.3.layer_norm1.bias\", \"text_model.encoder.layers.3.mlp.fc1.weight\", \"text_model.encoder.layers.3.mlp.fc1.bias\", \"text_model.encoder.layers.3.mlp.fc2.weight\", \"text_model.encoder.layers.3.mlp.fc2.bias\", \"text_model.encoder.layers.3.layer_norm2.weight\", \"text_model.encoder.layers.3.layer_norm2.bias\", \"text_model.encoder.layers.4.self_attn.k_proj.weight\", \"text_model.encoder.layers.4.self_attn.k_proj.bias\", \"text_model.encoder.layers.4.self_attn.v_proj.weight\", \"text_model.encoder.layers.4.self_attn.v_proj.bias\", \"text_model.encoder.layers.4.self_attn.q_proj.weight\", \"text_model.encoder.layers.4.self_attn.q_proj.bias\", \"text_model.encoder.layers.4.self_attn.out_proj.weight\", \"text_model.encoder.layers.4.self_attn.out_proj.bias\", \"text_model.encoder.layers.4.layer_norm1.weight\", \"text_model.encoder.layers.4.layer_norm1.bias\", \"text_model.encoder.layers.4.mlp.fc1.weight\", \"text_model.encoder.layers.4.mlp.fc1.bias\", \"text_model.encoder.layers.4.mlp.fc2.weight\", \"text_model.encoder.layers.4.mlp.fc2.bias\", \"text_model.encoder.layers.4.layer_norm2.weight\", \"text_model.encoder.layers.4.layer_norm2.bias\", \"text_model.encoder.layers.5.self_attn.k_proj.weight\", \"text_model.encoder.layers.5.self_attn.k_proj.bias\", \"text_model.encoder.layers.5.self_attn.v_proj.weight\", \"text_model.encoder.layers.5.self_attn.v_proj.bias\", \"text_model.encoder.layers.5.self_attn.q_proj.weight\", \"text_model.encoder.layers.5.self_attn.q_proj.bias\", \"text_model.encoder.layers.5.self_attn.out_proj.weight\", \"text_model.encoder.layers.5.self_attn.out_proj.bias\", \"text_model.encoder.layers.5.layer_norm1.weight\", \"text_model.encoder.layers.5.layer_norm1.bias\", \"text_model.encoder.layers.5.mlp.fc1.weight\", \"text_model.encoder.layers.5.mlp.fc1.bias\", \"text_model.encoder.layers.5.mlp.fc2.weight\", \"text_model.encoder.layers.5.mlp.fc2.bias\", \"text_model.encoder.layers.5.layer_norm2.weight\", \"text_model.encoder.layers.5.layer_norm2.bias\", \"text_model.encoder.layers.6.self_attn.k_proj.weight\", \"text_model.encoder.layers.6.self_attn.k_proj.bias\", \"text_model.encoder.layers.6.self_attn.v_proj.weight\", \"text_model.encoder.layers.6.self_attn.v_proj.bias\", \"text_model.encoder.layers.6.self_attn.q_proj.weight\", \"text_model.encoder.layers.6.self_attn.q_proj.bias\", \"text_model.encoder.layers.6.self_attn.out_proj.weight\", \"text_model.encoder.layers.6.self_attn.out_proj.bias\", \"text_model.encoder.layers.6.layer_norm1.weight\", \"text_model.encoder.layers.6.layer_norm1.bias\", \"text_model.encoder.layers.6.mlp.fc1.weight\", \"text_model.encoder.layers.6.mlp.fc1.bias\", \"text_model.encoder.layers.6.mlp.fc2.weight\", \"text_model.encoder.layers.6.mlp.fc2.bias\", \"text_model.encoder.layers.6.layer_norm2.weight\", \"text_model.encoder.layers.6.layer_norm2.bias\", \"text_model.encoder.layers.7.self_attn.k_proj.weight\", \"text_model.encoder.layers.7.self_attn.k_proj.bias\", \"text_model.encoder.layers.7.self_attn.v_proj.weight\", \"text_model.encoder.layers.7.self_attn.v_proj.bias\", \"text_model.encoder.layers.7.self_attn.q_proj.weight\", \"text_model.encoder.layers.7.self_attn.q_proj.bias\", \"text_model.encoder.layers.7.self_attn.out_proj.weight\", \"text_model.encoder.layers.7.self_attn.out_proj.bias\", \"text_model.encoder.layers.7.layer_norm1.weight\", \"text_model.encoder.layers.7.layer_norm1.bias\", \"text_model.encoder.layers.7.mlp.fc1.weight\", \"text_model.encoder.layers.7.mlp.fc1.bias\", \"text_model.encoder.layers.7.mlp.fc2.weight\", \"text_model.encoder.layers.7.mlp.fc2.bias\", \"text_model.encoder.layers.7.layer_norm2.weight\", \"text_model.encoder.layers.7.layer_norm2.bias\", \"text_model.encoder.layers.8.self_attn.k_proj.weight\", \"text_model.encoder.layers.8.self_attn.k_proj.bias\", \"text_model.encoder.layers.8.self_attn.v_proj.weight\", \"text_model.encoder.layers.8.self_attn.v_proj.bias\", \"text_model.encoder.layers.8.self_attn.q_proj.weight\", \"text_model.encoder.layers.8.self_attn.q_proj.bias\", \"text_model.encoder.layers.8.self_attn.out_proj.weight\", \"text_model.encoder.layers.8.self_attn.out_proj.bias\", \"text_model.encoder.layers.8.layer_norm1.weight\", \"text_model.encoder.layers.8.layer_norm1.bias\", \"text_model.encoder.layers.8.mlp.fc1.weight\", \"text_model.encoder.layers.8.mlp.fc1.bias\", \"text_model.encoder.layers.8.mlp.fc2.weight\", \"text_model.encoder.layers.8.mlp.fc2.bias\", \"text_model.encoder.layers.8.layer_norm2.weight\", \"text_model.encoder.layers.8.layer_norm2.bias\", \"text_model.encoder.layers.9.self_attn.k_proj.weight\", \"text_model.encoder.layers.9.self_attn.k_proj.bias\", \"text_model.encoder.layers.9.self_attn.v_proj.weight\", \"text_model.encoder.layers.9.self_attn.v_proj.bias\", \"text_model.encoder.layers.9.self_attn.q_proj.weight\", \"text_model.encoder.layers.9.self_attn.q_proj.bias\", \"text_model.encoder.layers.9.self_attn.out_proj.weight\", \"text_model.encoder.layers.9.self_attn.out_proj.bias\", \"text_model.encoder.layers.9.layer_norm1.weight\", \"text_model.encoder.layers.9.layer_norm1.bias\", \"text_model.encoder.layers.9.mlp.fc1.weight\", \"text_model.encoder.layers.9.mlp.fc1.bias\", \"text_model.encoder.layers.9.mlp.fc2.weight\", \"text_model.encoder.layers.9.mlp.fc2.bias\", \"text_model.encoder.layers.9.layer_norm2.weight\", \"text_model.encoder.layers.9.layer_norm2.bias\", \"text_model.encoder.layers.10.self_attn.k_proj.weight\", \"text_model.encoder.layers.10.self_attn.k_proj.bias\", \"text_model.encoder.layers.10.self_attn.v_proj.weight\", \"text_model.encoder.layers.10.self_attn.v_proj.bias\", \"text_model.encoder.layers.10.self_attn.q_proj.weight\", \"text_model.encoder.layers.10.self_attn.q_proj.bias\", \"text_model.encoder.layers.10.self_attn.out_proj.weight\", \"text_model.encoder.layers.10.self_attn.out_proj.bias\", \"text_model.encoder.layers.10.layer_norm1.weight\", \"text_model.encoder.layers.10.layer_norm1.bias\", \"text_model.encoder.layers.10.mlp.fc1.weight\", \"text_model.encoder.layers.10.mlp.fc1.bias\", \"text_model.encoder.layers.10.mlp.fc2.weight\", \"text_model.encoder.layers.10.mlp.fc2.bias\", \"text_model.encoder.layers.10.layer_norm2.weight\", \"text_model.encoder.layers.10.layer_norm2.bias\", \"text_model.encoder.layers.11.self_attn.k_proj.weight\", \"text_model.encoder.layers.11.self_attn.k_proj.bias\", \"text_model.encoder.layers.11.self_attn.v_proj.weight\", \"text_model.encoder.layers.11.self_attn.v_proj.bias\", \"text_model.encoder.layers.11.self_attn.q_proj.weight\", \"text_model.encoder.layers.11.self_attn.q_proj.bias\", \"text_model.encoder.layers.11.self_attn.out_proj.weight\", \"text_model.encoder.layers.11.self_attn.out_proj.bias\", \"text_model.encoder.layers.11.layer_norm1.weight\", \"text_model.encoder.layers.11.layer_norm1.bias\", \"text_model.encoder.layers.11.mlp.fc1.weight\", \"text_model.encoder.layers.11.mlp.fc1.bias\", \"text_model.encoder.layers.11.mlp.fc2.weight\", \"text_model.encoder.layers.11.mlp.fc2.bias\", \"text_model.encoder.layers.11.layer_norm2.weight\", \"text_model.encoder.layers.11.layer_norm2.bias\", \"text_model.final_layer_norm.weight\", \"text_model.final_layer_norm.bias\", \"vision_model.embeddings.class_embedding\", \"vision_model.embeddings.patch_embedding.weight\", \"vision_model.embeddings.position_embedding.weight\", \"vision_model.pre_layrnorm.weight\", \"vision_model.pre_layrnorm.bias\", \"vision_model.encoder.layers.0.self_attn.k_proj.weight\", \"vision_model.encoder.layers.0.self_attn.k_proj.bias\", \"vision_model.encoder.layers.0.self_attn.v_proj.weight\", \"vision_model.encoder.layers.0.self_attn.v_proj.bias\", \"vision_model.encoder.layers.0.self_attn.q_proj.weight\", \"vision_model.encoder.layers.0.self_attn.q_proj.bias\", \"vision_model.encoder.layers.0.self_attn.out_proj.weight\", \"vision_model.encoder.layers.0.self_attn.out_proj.bias\", \"vision_model.encoder.layers.0.layer_norm1.weight\", \"vision_model.encoder.layers.0.layer_norm1.bias\", \"vision_model.encoder.layers.0.mlp.fc1.weight\", \"vision_model.encoder.layers.0.mlp.fc1.bias\", \"vision_model.encoder.layers.0.mlp.fc2.weight\", \"vision_model.encoder.layers.0.mlp.fc2.bias\", \"vision_model.encoder.layers.0.layer_norm2.weight\", \"vision_model.encoder.layers.0.layer_norm2.bias\", \"vision_model.encoder.layers.1.self_attn.k_proj.weight\", \"vision_model.encoder.layers.1.self_attn.k_proj.bias\", \"vision_model.encoder.layers.1.self_attn.v_proj.weight\", \"vision_model.encoder.layers.1.self_attn.v_proj.bias\", \"vision_model.encoder.layers.1.self_attn.q_proj.weight\", \"vision_model.encoder.layers.1.self_attn.q_proj.bias\", \"vision_model.encoder.layers.1.self_attn.out_proj.weight\", \"vision_model.encoder.layers.1.self_attn.out_proj.bias\", \"vision_model.encoder.layers.1.layer_norm1.weight\", \"vision_model.encoder.layers.1.layer_norm1.bias\", \"vision_model.encoder.layers.1.mlp.fc1.weight\", \"vision_model.encoder.layers.1.mlp.fc1.bias\", \"vision_model.encoder.layers.1.mlp.fc2.weight\", \"vision_model.encoder.layers.1.mlp.fc2.bias\", \"vision_model.encoder.layers.1.layer_norm2.weight\", \"vision_model.encoder.layers.1.layer_norm2.bias\", \"vision_model.encoder.layers.2.self_attn.k_proj.weight\", \"vision_model.encoder.layers.2.self_attn.k_proj.bias\", \"vision_model.encoder.layers.2.self_attn.v_proj.weight\", \"vision_model.encoder.layers.2.self_attn.v_proj.bias\", \"vision_model.encoder.layers.2.self_attn.q_proj.weight\", \"vision_model.encoder.layers.2.self_attn.q_proj.bias\", \"vision_model.encoder.layers.2.self_attn.out_proj.weight\", \"vision_model.encoder.layers.2.self_attn.out_proj.bias\", \"vision_model.encoder.layers.2.layer_norm1.weight\", \"vision_model.encoder.layers.2.layer_norm1.bias\", \"vision_model.encoder.layers.2.mlp.fc1.weight\", \"vision_model.encoder.layers.2.mlp.fc1.bias\", \"vision_model.encoder.layers.2.mlp.fc2.weight\", \"vision_model.encoder.layers.2.mlp.fc2.bias\", \"vision_model.encoder.layers.2.layer_norm2.weight\", \"vision_model.encoder.layers.2.layer_norm2.bias\", \"vision_model.encoder.layers.3.self_attn.k_proj.weight\", \"vision_model.encoder.layers.3.self_attn.k_proj.bias\", \"vision_model.encoder.layers.3.self_attn.v_proj.weight\", \"vision_model.encoder.layers.3.self_attn.v_proj.bias\", \"vision_model.encoder.layers.3.self_attn.q_proj.weight\", \"vision_model.encoder.layers.3.self_attn.q_proj.bias\", \"vision_model.encoder.layers.3.self_attn.out_proj.weight\", \"vision_model.encoder.layers.3.self_attn.out_proj.bias\", \"vision_model.encoder.layers.3.layer_norm1.weight\", \"vision_model.encoder.layers.3.layer_norm1.bias\", \"vision_model.encoder.layers.3.mlp.fc1.weight\", \"vision_model.encoder.layers.3.mlp.fc1.bias\", \"vision_model.encoder.layers.3.mlp.fc2.weight\", \"vision_model.encoder.layers.3.mlp.fc2.bias\", \"vision_model.encoder.layers.3.layer_norm2.weight\", \"vision_model.encoder.layers.3.layer_norm2.bias\", \"vision_model.encoder.layers.4.self_attn.k_proj.weight\", \"vision_model.encoder.layers.4.self_attn.k_proj.bias\", \"vision_model.encoder.layers.4.self_attn.v_proj.weight\", \"vision_model.encoder.layers.4.self_attn.v_proj.bias\", \"vision_model.encoder.layers.4.self_attn.q_proj.weight\", \"vision_model.encoder.layers.4.self_attn.q_proj.bias\", \"vision_model.encoder.layers.4.self_attn.out_proj.weight\", \"vision_model.encoder.layers.4.self_attn.out_proj.bias\", \"vision_model.encoder.layers.4.layer_norm1.weight\", \"vision_model.encoder.layers.4.layer_norm1.bias\", \"vision_model.encoder.layers.4.mlp.fc1.weight\", \"vision_model.encoder.layers.4.mlp.fc1.bias\", \"vision_model.encoder.layers.4.mlp.fc2.weight\", \"vision_model.encoder.layers.4.mlp.fc2.bias\", \"vision_model.encoder.layers.4.layer_norm2.weight\", \"vision_model.encoder.layers.4.layer_norm2.bias\", \"vision_model.encoder.layers.5.self_attn.k_proj.weight\", \"vision_model.encoder.layers.5.self_attn.k_proj.bias\", \"vision_model.encoder.layers.5.self_attn.v_proj.weight\", \"vision_model.encoder.layers.5.self_attn.v_proj.bias\", \"vision_model.encoder.layers.5.self_attn.q_proj.weight\", \"vision_model.encoder.layers.5.self_attn.q_proj.bias\", \"vision_model.encoder.layers.5.self_attn.out_proj.weight\", \"vision_model.encoder.layers.5.self_attn.out_proj.bias\", \"vision_model.encoder.layers.5.layer_norm1.weight\", \"vision_model.encoder.layers.5.layer_norm1.bias\", \"vision_model.encoder.layers.5.mlp.fc1.weight\", \"vision_model.encoder.layers.5.mlp.fc1.bias\", \"vision_model.encoder.layers.5.mlp.fc2.weight\", \"vision_model.encoder.layers.5.mlp.fc2.bias\", \"vision_model.encoder.layers.5.layer_norm2.weight\", \"vision_model.encoder.layers.5.layer_norm2.bias\", \"vision_model.encoder.layers.6.self_attn.k_proj.weight\", \"vision_model.encoder.layers.6.self_attn.k_proj.bias\", \"vision_model.encoder.layers.6.self_attn.v_proj.weight\", \"vision_model.encoder.layers.6.self_attn.v_proj.bias\", \"vision_model.encoder.layers.6.self_attn.q_proj.weight\", \"vision_model.encoder.layers.6.self_attn.q_proj.bias\", \"vision_model.encoder.layers.6.self_attn.out_proj.weight\", \"vision_model.encoder.layers.6.self_attn.out_proj.bias\", \"vision_model.encoder.layers.6.layer_norm1.weight\", \"vision_model.encoder.layers.6.layer_norm1.bias\", \"vision_model.encoder.layers.6.mlp.fc1.weight\", \"vision_model.encoder.layers.6.mlp.fc1.bias\", \"vision_model.encoder.layers.6.mlp.fc2.weight\", \"vision_model.encoder.layers.6.mlp.fc2.bias\", \"vision_model.encoder.layers.6.layer_norm2.weight\", \"vision_model.encoder.layers.6.layer_norm2.bias\", \"vision_model.encoder.layers.7.self_attn.k_proj.weight\", \"vision_model.encoder.layers.7.self_attn.k_proj.bias\", \"vision_model.encoder.layers.7.self_attn.v_proj.weight\", \"vision_model.encoder.layers.7.self_attn.v_proj.bias\", \"vision_model.encoder.layers.7.self_attn.q_proj.weight\", \"vision_model.encoder.layers.7.self_attn.q_proj.bias\", \"vision_model.encoder.layers.7.self_attn.out_proj.weight\", \"vision_model.encoder.layers.7.self_attn.out_proj.bias\", \"vision_model.encoder.layers.7.layer_norm1.weight\", \"vision_model.encoder.layers.7.layer_norm1.bias\", \"vision_model.encoder.layers.7.mlp.fc1.weight\", \"vision_model.encoder.layers.7.mlp.fc1.bias\", \"vision_model.encoder.layers.7.mlp.fc2.weight\", \"vision_model.encoder.layers.7.mlp.fc2.bias\", \"vision_model.encoder.layers.7.layer_norm2.weight\", \"vision_model.encoder.layers.7.layer_norm2.bias\", \"vision_model.encoder.layers.8.self_attn.k_proj.weight\", \"vision_model.encoder.layers.8.self_attn.k_proj.bias\", \"vision_model.encoder.layers.8.self_attn.v_proj.weight\", \"vision_model.encoder.layers.8.self_attn.v_proj.bias\", \"vision_model.encoder.layers.8.self_attn.q_proj.weight\", \"vision_model.encoder.layers.8.self_attn.q_proj.bias\", \"vision_model.encoder.layers.8.self_attn.out_proj.weight\", \"vision_model.encoder.layers.8.self_attn.out_proj.bias\", \"vision_model.encoder.layers.8.layer_norm1.weight\", \"vision_model.encoder.layers.8.layer_norm1.bias\", \"vision_model.encoder.layers.8.mlp.fc1.weight\", \"vision_model.encoder.layers.8.mlp.fc1.bias\", \"vision_model.encoder.layers.8.mlp.fc2.weight\", \"vision_model.encoder.layers.8.mlp.fc2.bias\", \"vision_model.encoder.layers.8.layer_norm2.weight\", \"vision_model.encoder.layers.8.layer_norm2.bias\", \"vision_model.encoder.layers.9.self_attn.k_proj.weight\", \"vision_model.encoder.layers.9.self_attn.k_proj.bias\", \"vision_model.encoder.layers.9.self_attn.v_proj.weight\", \"vision_model.encoder.layers.9.self_attn.v_proj.bias\", \"vision_model.encoder.layers.9.self_attn.q_proj.weight\", \"vision_model.encoder.layers.9.self_attn.q_proj.bias\", \"vision_model.encoder.layers.9.self_attn.out_proj.weight\", \"vision_model.encoder.layers.9.self_attn.out_proj.bias\", \"vision_model.encoder.layers.9.layer_norm1.weight\", \"vision_model.encoder.layers.9.layer_norm1.bias\", \"vision_model.encoder.layers.9.mlp.fc1.weight\", \"vision_model.encoder.layers.9.mlp.fc1.bias\", \"vision_model.encoder.layers.9.mlp.fc2.weight\", \"vision_model.encoder.layers.9.mlp.fc2.bias\", \"vision_model.encoder.layers.9.layer_norm2.weight\", \"vision_model.encoder.layers.9.layer_norm2.bias\", \"vision_model.encoder.layers.10.self_attn.k_proj.weight\", \"vision_model.encoder.layers.10.self_attn.k_proj.bias\", \"vision_model.encoder.layers.10.self_attn.v_proj.weight\", \"vision_model.encoder.layers.10.self_attn.v_proj.bias\", \"vision_model.encoder.layers.10.self_attn.q_proj.weight\", \"vision_model.encoder.layers.10.self_attn.q_proj.bias\", \"vision_model.encoder.layers.10.self_attn.out_proj.weight\", \"vision_model.encoder.layers.10.self_attn.out_proj.bias\", \"vision_model.encoder.layers.10.layer_norm1.weight\", \"vision_model.encoder.layers.10.layer_norm1.bias\", \"vision_model.encoder.layers.10.mlp.fc1.weight\", \"vision_model.encoder.layers.10.mlp.fc1.bias\", \"vision_model.encoder.layers.10.mlp.fc2.weight\", \"vision_model.encoder.layers.10.mlp.fc2.bias\", \"vision_model.encoder.layers.10.layer_norm2.weight\", \"vision_model.encoder.layers.10.layer_norm2.bias\", \"vision_model.encoder.layers.11.self_attn.k_proj.weight\", \"vision_model.encoder.layers.11.self_attn.k_proj.bias\", \"vision_model.encoder.layers.11.self_attn.v_proj.weight\", \"vision_model.encoder.layers.11.self_attn.v_proj.bias\", \"vision_model.encoder.layers.11.self_attn.q_proj.weight\", \"vision_model.encoder.layers.11.self_attn.q_proj.bias\", \"vision_model.encoder.layers.11.self_attn.out_proj.weight\", \"vision_model.encoder.layers.11.self_attn.out_proj.bias\", \"vision_model.encoder.layers.11.layer_norm1.weight\", \"vision_model.encoder.layers.11.layer_norm1.bias\", \"vision_model.encoder.layers.11.mlp.fc1.weight\", \"vision_model.encoder.layers.11.mlp.fc1.bias\", \"vision_model.encoder.layers.11.mlp.fc2.weight\", \"vision_model.encoder.layers.11.mlp.fc2.bias\", \"vision_model.encoder.layers.11.layer_norm2.weight\", \"vision_model.encoder.layers.11.layer_norm2.bias\", \"vision_model.post_layernorm.weight\", \"vision_model.post_layernorm.bias\", \"visual_projection.weight\", \"text_projection.weight\". \n",
      "\tUnexpected key(s) in state_dict: \"module.clip.logit_scale\", \"module.clip.text_model.embeddings.token_embedding.weight\", \"module.clip.text_model.embeddings.position_embedding.weight\", \"module.clip.text_model.encoder.layers.0.self_attn.k_proj.weight\", \"module.clip.text_model.encoder.layers.0.self_attn.k_proj.bias\", \"module.clip.text_model.encoder.layers.0.self_attn.v_proj.weight\", \"module.clip.text_model.encoder.layers.0.self_attn.v_proj.bias\", \"module.clip.text_model.encoder.layers.0.self_attn.q_proj.weight\", \"module.clip.text_model.encoder.layers.0.self_attn.q_proj.bias\", \"module.clip.text_model.encoder.layers.0.self_attn.out_proj.weight\", \"module.clip.text_model.encoder.layers.0.self_attn.out_proj.bias\", \"module.clip.text_model.encoder.layers.0.layer_norm1.weight\", \"module.clip.text_model.encoder.layers.0.layer_norm1.bias\", \"module.clip.text_model.encoder.layers.0.mlp.fc1.weight\", \"module.clip.text_model.encoder.layers.0.mlp.fc1.bias\", \"module.clip.text_model.encoder.layers.0.mlp.fc2.weight\", \"module.clip.text_model.encoder.layers.0.mlp.fc2.bias\", \"module.clip.text_model.encoder.layers.0.layer_norm2.weight\", \"module.clip.text_model.encoder.layers.0.layer_norm2.bias\", \"module.clip.text_model.encoder.layers.1.self_attn.k_proj.weight\", \"module.clip.text_model.encoder.layers.1.self_attn.k_proj.bias\", \"module.clip.text_model.encoder.layers.1.self_attn.v_proj.weight\", \"module.clip.text_model.encoder.layers.1.self_attn.v_proj.bias\", \"module.clip.text_model.encoder.layers.1.self_attn.q_proj.weight\", \"module.clip.text_model.encoder.layers.1.self_attn.q_proj.bias\", \"module.clip.text_model.encoder.layers.1.self_attn.out_proj.weight\", \"module.clip.text_model.encoder.layers.1.self_attn.out_proj.bias\", \"module.clip.text_model.encoder.layers.1.layer_norm1.weight\", \"module.clip.text_model.encoder.layers.1.layer_norm1.bias\", \"module.clip.text_model.encoder.layers.1.mlp.fc1.weight\", \"module.clip.text_model.encoder.layers.1.mlp.fc1.bias\", \"module.clip.text_model.encoder.layers.1.mlp.fc2.weight\", \"module.clip.text_model.encoder.layers.1.mlp.fc2.bias\", \"module.clip.text_model.encoder.layers.1.layer_norm2.weight\", \"module.clip.text_model.encoder.layers.1.layer_norm2.bias\", \"module.clip.text_model.encoder.layers.2.self_attn.k_proj.weight\", \"module.clip.text_model.encoder.layers.2.self_attn.k_proj.bias\", \"module.clip.text_model.encoder.layers.2.self_attn.v_proj.weight\", \"module.clip.text_model.encoder.layers.2.self_attn.v_proj.bias\", \"module.clip.text_model.encoder.layers.2.self_attn.q_proj.weight\", \"module.clip.text_model.encoder.layers.2.self_attn.q_proj.bias\", \"module.clip.text_model.encoder.layers.2.self_attn.out_proj.weight\", \"module.clip.text_model.encoder.layers.2.self_attn.out_proj.bias\", \"module.clip.text_model.encoder.layers.2.layer_norm1.weight\", \"module.clip.text_model.encoder.layers.2.layer_norm1.bias\", \"module.clip.text_model.encoder.layers.2.mlp.fc1.weight\", \"module.clip.text_model.encoder.layers.2.mlp.fc1.bias\", \"module.clip.text_model.encoder.layers.2.mlp.fc2.weight\", \"module.clip.text_model.encoder.layers.2.mlp.fc2.bias\", \"module.clip.text_model.encoder.layers.2.layer_norm2.weight\", \"module.clip.text_model.encoder.layers.2.layer_norm2.bias\", \"module.clip.text_model.encoder.layers.3.self_attn.k_proj.weight\", \"module.clip.text_model.encoder.layers.3.self_attn.k_proj.bias\", \"module.clip.text_model.encoder.layers.3.self_attn.v_proj.weight\", \"module.clip.text_model.encoder.layers.3.self_attn.v_proj.bias\", \"module.clip.text_model.encoder.layers.3.self_attn.q_proj.weight\", \"module.clip.text_model.encoder.layers.3.self_attn.q_proj.bias\", \"module.clip.text_model.encoder.layers.3.self_attn.out_proj.weight\", \"module.clip.text_model.encoder.layers.3.self_attn.out_proj.bias\", \"module.clip.text_model.encoder.layers.3.layer_norm1.weight\", \"module.clip.text_model.encoder.layers.3.layer_norm1.bias\", \"module.clip.text_model.encoder.layers.3.mlp.fc1.weight\", \"module.clip.text_model.encoder.layers.3.mlp.fc1.bias\", \"module.clip.text_model.encoder.layers.3.mlp.fc2.weight\", \"module.clip.text_model.encoder.layers.3.mlp.fc2.bias\", \"module.clip.text_model.encoder.layers.3.layer_norm2.weight\", \"module.clip.text_model.encoder.layers.3.layer_norm2.bias\", \"module.clip.text_model.encoder.layers.4.self_attn.k_proj.weight\", \"module.clip.text_model.encoder.layers.4.self_attn.k_proj.bias\", \"module.clip.text_model.encoder.layers.4.self_attn.v_proj.weight\", \"module.clip.text_model.encoder.layers.4.self_attn.v_proj.bias\", \"module.clip.text_model.encoder.layers.4.self_attn.q_proj.weight\", \"module.clip.text_model.encoder.layers.4.self_attn.q_proj.bias\", \"module.clip.text_model.encoder.layers.4.self_attn.out_proj.weight\", \"module.clip.text_model.encoder.layers.4.self_attn.out_proj.bias\", \"module.clip.text_model.encoder.layers.4.layer_norm1.weight\", \"module.clip.text_model.encoder.layers.4.layer_norm1.bias\", \"module.clip.text_model.encoder.layers.4.mlp.fc1.weight\", \"module.clip.text_model.encoder.layers.4.mlp.fc1.bias\", \"module.clip.text_model.encoder.layers.4.mlp.fc2.weight\", \"module.clip.text_model.encoder.layers.4.mlp.fc2.bias\", \"module.clip.text_model.encoder.layers.4.layer_norm2.weight\", \"module.clip.text_model.encoder.layers.4.layer_norm2.bias\", \"module.clip.text_model.encoder.layers.5.self_attn.k_proj.weight\", \"module.clip.text_model.encoder.layers.5.self_attn.k_proj.bias\", \"module.clip.text_model.encoder.layers.5.self_attn.v_proj.weight\", \"module.clip.text_model.encoder.layers.5.self_attn.v_proj.bias\", \"module.clip.text_model.encoder.layers.5.self_attn.q_proj.weight\", \"module.clip.text_model.encoder.layers.5.self_attn.q_proj.bias\", \"module.clip.text_model.encoder.layers.5.self_attn.out_proj.weight\", \"module.clip.text_model.encoder.layers.5.self_attn.out_proj.bias\", \"module.clip.text_model.encoder.layers.5.layer_norm1.weight\", \"module.clip.text_model.encoder.layers.5.layer_norm1.bias\", \"module.clip.text_model.encoder.layers.5.mlp.fc1.weight\", \"module.clip.text_model.encoder.layers.5.mlp.fc1.bias\", \"module.clip.text_model.encoder.layers.5.mlp.fc2.weight\", \"module.clip.text_model.encoder.layers.5.mlp.fc2.bias\", \"module.clip.text_model.encoder.layers.5.layer_norm2.weight\", \"module.clip.text_model.encoder.layers.5.layer_norm2.bias\", \"module.clip.text_model.encoder.layers.6.self_attn.k_proj.weight\", \"module.clip.text_model.encoder.layers.6.self_attn.k_proj.bias\", \"module.clip.text_model.encoder.layers.6.self_attn.v_proj.weight\", \"module.clip.text_model.encoder.layers.6.self_attn.v_proj.bias\", \"module.clip.text_model.encoder.layers.6.self_attn.q_proj.weight\", \"module.clip.text_model.encoder.layers.6.self_attn.q_proj.bias\", \"module.clip.text_model.encoder.layers.6.self_attn.out_proj.weight\", \"module.clip.text_model.encoder.layers.6.self_attn.out_proj.bias\", \"module.clip.text_model.encoder.layers.6.layer_norm1.weight\", \"module.clip.text_model.encoder.layers.6.layer_norm1.bias\", \"module.clip.text_model.encoder.layers.6.mlp.fc1.weight\", \"module.clip.text_model.encoder.layers.6.mlp.fc1.bias\", \"module.clip.text_model.encoder.layers.6.mlp.fc2.weight\", \"module.clip.text_model.encoder.layers.6.mlp.fc2.bias\", \"module.clip.text_model.encoder.layers.6.layer_norm2.weight\", \"module.clip.text_model.encoder.layers.6.layer_norm2.bias\", \"module.clip.text_model.encoder.layers.7.self_attn.k_proj.weight\", \"module.clip.text_model.encoder.layers.7.self_attn.k_proj.bias\", \"module.clip.text_model.encoder.layers.7.self_attn.v_proj.weight\", \"module.clip.text_model.encoder.layers.7.self_attn.v_proj.bias\", \"module.clip.text_model.encoder.layers.7.self_attn.q_proj.weight\", \"module.clip.text_model.encoder.layers.7.self_attn.q_proj.bias\", \"module.clip.text_model.encoder.layers.7.self_attn.out_proj.weight\", \"module.clip.text_model.encoder.layers.7.self_attn.out_proj.bias\", \"module.clip.text_model.encoder.layers.7.layer_norm1.weight\", \"module.clip.text_model.encoder.layers.7.layer_norm1.bias\", \"module.clip.text_model.encoder.layers.7.mlp.fc1.weight\", \"module.clip.text_model.encoder.layers.7.mlp.fc1.bias\", \"module.clip.text_model.encoder.layers.7.mlp.fc2.weight\", \"module.clip.text_model.encoder.layers.7.mlp.fc2.bias\", \"module.clip.text_model.encoder.layers.7.layer_norm2.weight\", \"module.clip.text_model.encoder.layers.7.layer_norm2.bias\", \"module.clip.text_model.encoder.layers.8.self_attn.k_proj.weight\", \"module.clip.text_model.encoder.layers.8.self_attn.k_proj.bias\", \"module.clip.text_model.encoder.layers.8.self_attn.v_proj.weight\", \"module.clip.text_model.encoder.layers.8.self_attn.v_proj.bias\", \"module.clip.text_model.encoder.layers.8.self_attn.q_proj.weight\", \"module.clip.text_model.encoder.layers.8.self_attn.q_proj.bias\", \"module.clip.text_model.encoder.layers.8.self_attn.out_proj.weight\", \"module.clip.text_model.encoder.layers.8.self_attn.out_proj.bias\", \"module.clip.text_model.encoder.layers.8.layer_norm1.weight\", \"module.clip.text_model.encoder.layers.8.layer_norm1.bias\", \"module.clip.text_model.encoder.layers.8.mlp.fc1.weight\", \"module.clip.text_model.encoder.layers.8.mlp.fc1.bias\", \"module.clip.text_model.encoder.layers.8.mlp.fc2.weight\", \"module.clip.text_model.encoder.layers.8.mlp.fc2.bias\", \"module.clip.text_model.encoder.layers.8.layer_norm2.weight\", \"module.clip.text_model.encoder.layers.8.layer_norm2.bias\", \"module.clip.text_model.encoder.layers.9.self_attn.k_proj.weight\", \"module.clip.text_model.encoder.layers.9.self_attn.k_proj.bias\", \"module.clip.text_model.encoder.layers.9.self_attn.v_proj.weight\", \"module.clip.text_model.encoder.layers.9.self_attn.v_proj.bias\", \"module.clip.text_model.encoder.layers.9.self_attn.q_proj.weight\", \"module.clip.text_model.encoder.layers.9.self_attn.q_proj.bias\", \"module.clip.text_model.encoder.layers.9.self_attn.out_proj.weight\", \"module.clip.text_model.encoder.layers.9.self_attn.out_proj.bias\", \"module.clip.text_model.encoder.layers.9.layer_norm1.weight\", \"module.clip.text_model.encoder.layers.9.layer_norm1.bias\", \"module.clip.text_model.encoder.layers.9.mlp.fc1.weight\", \"module.clip.text_model.encoder.layers.9.mlp.fc1.bias\", \"module.clip.text_model.encoder.layers.9.mlp.fc2.weight\", \"module.clip.text_model.encoder.layers.9.mlp.fc2.bias\", \"module.clip.text_model.encoder.layers.9.layer_norm2.weight\", \"module.clip.text_model.encoder.layers.9.layer_norm2.bias\", \"module.clip.text_model.encoder.layers.10.self_attn.k_proj.weight\", \"module.clip.text_model.encoder.layers.10.self_attn.k_proj.bias\", \"module.clip.text_model.encoder.layers.10.self_attn.v_proj.weight\", \"module.clip.text_model.encoder.layers.10.self_attn.v_proj.bias\", \"module.clip.text_model.encoder.layers.10.self_attn.q_proj.weight\", \"module.clip.text_model.encoder.layers.10.self_attn.q_proj.bias\", \"module.clip.text_model.encoder.layers.10.self_attn.out_proj.weight\", \"module.clip.text_model.encoder.layers.10.self_attn.out_proj.bias\", \"module.clip.text_model.encoder.layers.10.layer_norm1.weight\", \"module.clip.text_model.encoder.layers.10.layer_norm1.bias\", \"module.clip.text_model.encoder.layers.10.mlp.fc1.weight\", \"module.clip.text_model.encoder.layers.10.mlp.fc1.bias\", \"module.clip.text_model.encoder.layers.10.mlp.fc2.weight\", \"module.clip.text_model.encoder.layers.10.mlp.fc2.bias\", \"module.clip.text_model.encoder.layers.10.layer_norm2.weight\", \"module.clip.text_model.encoder.layers.10.layer_norm2.bias\", \"module.clip.text_model.encoder.layers.11.self_attn.k_proj.weight\", \"module.clip.text_model.encoder.layers.11.self_attn.k_proj.bias\", \"module.clip.text_model.encoder.layers.11.self_attn.v_proj.weight\", \"module.clip.text_model.encoder.layers.11.self_attn.v_proj.bias\", \"module.clip.text_model.encoder.layers.11.self_attn.q_proj.weight\", \"module.clip.text_model.encoder.layers.11.self_attn.q_proj.bias\", \"module.clip.text_model.encoder.layers.11.self_attn.out_proj.weight\", \"module.clip.text_model.encoder.layers.11.self_attn.out_proj.bias\", \"module.clip.text_model.encoder.layers.11.layer_norm1.weight\", \"module.clip.text_model.encoder.layers.11.layer_norm1.bias\", \"module.clip.text_model.encoder.layers.11.mlp.fc1.weight\", \"module.clip.text_model.encoder.layers.11.mlp.fc1.bias\", \"module.clip.text_model.encoder.layers.11.mlp.fc2.weight\", \"module.clip.text_model.encoder.layers.11.mlp.fc2.bias\", \"module.clip.text_model.encoder.layers.11.layer_norm2.weight\", \"module.clip.text_model.encoder.layers.11.layer_norm2.bias\", \"module.clip.text_model.final_layer_norm.weight\", \"module.clip.text_model.final_layer_norm.bias\", \"module.clip.vision_model.embeddings.class_embedding\", \"module.clip.vision_model.embeddings.patch_embedding.weight\", \"module.clip.vision_model.embeddings.position_embedding.weight\", \"module.clip.vision_model.pre_layrnorm.weight\", \"module.clip.vision_model.pre_layrnorm.bias\", \"module.clip.vision_model.encoder.layers.0.self_attn.k_proj.weight\", \"module.clip.vision_model.encoder.layers.0.self_attn.k_proj.bias\", \"module.clip.vision_model.encoder.layers.0.self_attn.v_proj.weight\", \"module.clip.vision_model.encoder.layers.0.self_attn.v_proj.bias\", \"module.clip.vision_model.encoder.layers.0.self_attn.q_proj.weight\", \"module.clip.vision_model.encoder.layers.0.self_attn.q_proj.bias\", \"module.clip.vision_model.encoder.layers.0.self_attn.out_proj.weight\", \"module.clip.vision_model.encoder.layers.0.self_attn.out_proj.bias\", \"module.clip.vision_model.encoder.layers.0.layer_norm1.weight\", \"module.clip.vision_model.encoder.layers.0.layer_norm1.bias\", \"module.clip.vision_model.encoder.layers.0.mlp.fc1.weight\", \"module.clip.vision_model.encoder.layers.0.mlp.fc1.bias\", \"module.clip.vision_model.encoder.layers.0.mlp.fc2.weight\", \"module.clip.vision_model.encoder.layers.0.mlp.fc2.bias\", \"module.clip.vision_model.encoder.layers.0.layer_norm2.weight\", \"module.clip.vision_model.encoder.layers.0.layer_norm2.bias\", \"module.clip.vision_model.encoder.layers.1.self_attn.k_proj.weight\", \"module.clip.vision_model.encoder.layers.1.self_attn.k_proj.bias\", \"module.clip.vision_model.encoder.layers.1.self_attn.v_proj.weight\", \"module.clip.vision_model.encoder.layers.1.self_attn.v_proj.bias\", \"module.clip.vision_model.encoder.layers.1.self_attn.q_proj.weight\", \"module.clip.vision_model.encoder.layers.1.self_attn.q_proj.bias\", \"module.clip.vision_model.encoder.layers.1.self_attn.out_proj.weight\", \"module.clip.vision_model.encoder.layers.1.self_attn.out_proj.bias\", \"module.clip.vision_model.encoder.layers.1.layer_norm1.weight\", \"module.clip.vision_model.encoder.layers.1.layer_norm1.bias\", \"module.clip.vision_model.encoder.layers.1.mlp.fc1.weight\", \"module.clip.vision_model.encoder.layers.1.mlp.fc1.bias\", \"module.clip.vision_model.encoder.layers.1.mlp.fc2.weight\", \"module.clip.vision_model.encoder.layers.1.mlp.fc2.bias\", \"module.clip.vision_model.encoder.layers.1.layer_norm2.weight\", \"module.clip.vision_model.encoder.layers.1.layer_norm2.bias\", \"module.clip.vision_model.encoder.layers.2.self_attn.k_proj.weight\", \"module.clip.vision_model.encoder.layers.2.self_attn.k_proj.bias\", \"module.clip.vision_model.encoder.layers.2.self_attn.v_proj.weight\", \"module.clip.vision_model.encoder.layers.2.self_attn.v_proj.bias\", \"module.clip.vision_model.encoder.layers.2.self_attn.q_proj.weight\", \"module.clip.vision_model.encoder.layers.2.self_attn.q_proj.bias\", \"module.clip.vision_model.encoder.layers.2.self_attn.out_proj.weight\", \"module.clip.vision_model.encoder.layers.2.self_attn.out_proj.bias\", \"module.clip.vision_model.encoder.layers.2.layer_norm1.weight\", \"module.clip.vision_model.encoder.layers.2.layer_norm1.bias\", \"module.clip.vision_model.encoder.layers.2.mlp.fc1.weight\", \"module.clip.vision_model.encoder.layers.2.mlp.fc1.bias\", \"module.clip.vision_model.encoder.layers.2.mlp.fc2.weight\", \"module.clip.vision_model.encoder.layers.2.mlp.fc2.bias\", \"module.clip.vision_model.encoder.layers.2.layer_norm2.weight\", \"module.clip.vision_model.encoder.layers.2.layer_norm2.bias\", \"module.clip.vision_model.encoder.layers.3.self_attn.k_proj.weight\", \"module.clip.vision_model.encoder.layers.3.self_attn.k_proj.bias\", \"module.clip.vision_model.encoder.layers.3.self_attn.v_proj.weight\", \"module.clip.vision_model.encoder.layers.3.self_attn.v_proj.bias\", \"module.clip.vision_model.encoder.layers.3.self_attn.q_proj.weight\", \"module.clip.vision_model.encoder.layers.3.self_attn.q_proj.bias\", \"module.clip.vision_model.encoder.layers.3.self_attn.out_proj.weight\", \"module.clip.vision_model.encoder.layers.3.self_attn.out_proj.bias\", \"module.clip.vision_model.encoder.layers.3.layer_norm1.weight\", \"module.clip.vision_model.encoder.layers.3.layer_norm1.bias\", \"module.clip.vision_model.encoder.layers.3.mlp.fc1.weight\", \"module.clip.vision_model.encoder.layers.3.mlp.fc1.bias\", \"module.clip.vision_model.encoder.layers.3.mlp.fc2.weight\", \"module.clip.vision_model.encoder.layers.3.mlp.fc2.bias\", \"module.clip.vision_model.encoder.layers.3.layer_norm2.weight\", \"module.clip.vision_model.encoder.layers.3.layer_norm2.bias\", \"module.clip.vision_model.encoder.layers.4.self_attn.k_proj.weight\", \"module.clip.vision_model.encoder.layers.4.self_attn.k_proj.bias\", \"module.clip.vision_model.encoder.layers.4.self_attn.v_proj.weight\", \"module.clip.vision_model.encoder.layers.4.self_attn.v_proj.bias\", \"module.clip.vision_model.encoder.layers.4.self_attn.q_proj.weight\", \"module.clip.vision_model.encoder.layers.4.self_attn.q_proj.bias\", \"module.clip.vision_model.encoder.layers.4.self_attn.out_proj.weight\", \"module.clip.vision_model.encoder.layers.4.self_attn.out_proj.bias\", \"module.clip.vision_model.encoder.layers.4.layer_norm1.weight\", \"module.clip.vision_model.encoder.layers.4.layer_norm1.bias\", \"module.clip.vision_model.encoder.layers.4.mlp.fc1.weight\", \"module.clip.vision_model.encoder.layers.4.mlp.fc1.bias\", \"module.clip.vision_model.encoder.layers.4.mlp.fc2.weight\", \"module.clip.vision_model.encoder.layers.4.mlp.fc2.bias\", \"module.clip.vision_model.encoder.layers.4.layer_norm2.weight\", \"module.clip.vision_model.encoder.layers.4.layer_norm2.bias\", \"module.clip.vision_model.encoder.layers.5.self_attn.k_proj.weight\", \"module.clip.vision_model.encoder.layers.5.self_attn.k_proj.bias\", \"module.clip.vision_model.encoder.layers.5.self_attn.v_proj.weight\", \"module.clip.vision_model.encoder.layers.5.self_attn.v_proj.bias\", \"module.clip.vision_model.encoder.layers.5.self_attn.q_proj.weight\", \"module.clip.vision_model.encoder.layers.5.self_attn.q_proj.bias\", \"module.clip.vision_model.encoder.layers.5.self_attn.out_proj.weight\", \"module.clip.vision_model.encoder.layers.5.self_attn.out_proj.bias\", \"module.clip.vision_model.encoder.layers.5.layer_norm1.weight\", \"module.clip.vision_model.encoder.layers.5.layer_norm1.bias\", \"module.clip.vision_model.encoder.layers.5.mlp.fc1.weight\", \"module.clip.vision_model.encoder.layers.5.mlp.fc1.bias\", \"module.clip.vision_model.encoder.layers.5.mlp.fc2.weight\", \"module.clip.vision_model.encoder.layers.5.mlp.fc2.bias\", \"module.clip.vision_model.encoder.layers.5.layer_norm2.weight\", \"module.clip.vision_model.encoder.layers.5.layer_norm2.bias\", \"module.clip.vision_model.encoder.layers.6.self_attn.k_proj.weight\", \"module.clip.vision_model.encoder.layers.6.self_attn.k_proj.bias\", \"module.clip.vision_model.encoder.layers.6.self_attn.v_proj.weight\", \"module.clip.vision_model.encoder.layers.6.self_attn.v_proj.bias\", \"module.clip.vision_model.encoder.layers.6.self_attn.q_proj.weight\", \"module.clip.vision_model.encoder.layers.6.self_attn.q_proj.bias\", \"module.clip.vision_model.encoder.layers.6.self_attn.out_proj.weight\", \"module.clip.vision_model.encoder.layers.6.self_attn.out_proj.bias\", \"module.clip.vision_model.encoder.layers.6.layer_norm1.weight\", \"module.clip.vision_model.encoder.layers.6.layer_norm1.bias\", \"module.clip.vision_model.encoder.layers.6.mlp.fc1.weight\", \"module.clip.vision_model.encoder.layers.6.mlp.fc1.bias\", \"module.clip.vision_model.encoder.layers.6.mlp.fc2.weight\", \"module.clip.vision_model.encoder.layers.6.mlp.fc2.bias\", \"module.clip.vision_model.encoder.layers.6.layer_norm2.weight\", \"module.clip.vision_model.encoder.layers.6.layer_norm2.bias\", \"module.clip.vision_model.encoder.layers.7.self_attn.k_proj.weight\", \"module.clip.vision_model.encoder.layers.7.self_attn.k_proj.bias\", \"module.clip.vision_model.encoder.layers.7.self_attn.v_proj.weight\", \"module.clip.vision_model.encoder.layers.7.self_attn.v_proj.bias\", \"module.clip.vision_model.encoder.layers.7.self_attn.q_proj.weight\", \"module.clip.vision_model.encoder.layers.7.self_attn.q_proj.bias\", \"module.clip.vision_model.encoder.layers.7.self_attn.out_proj.weight\", \"module.clip.vision_model.encoder.layers.7.self_attn.out_proj.bias\", \"module.clip.vision_model.encoder.layers.7.layer_norm1.weight\", \"module.clip.vision_model.encoder.layers.7.layer_norm1.bias\", \"module.clip.vision_model.encoder.layers.7.mlp.fc1.weight\", \"module.clip.vision_model.encoder.layers.7.mlp.fc1.bias\", \"module.clip.vision_model.encoder.layers.7.mlp.fc2.weight\", \"module.clip.vision_model.encoder.layers.7.mlp.fc2.bias\", \"module.clip.vision_model.encoder.layers.7.layer_norm2.weight\", \"module.clip.vision_model.encoder.layers.7.layer_norm2.bias\", \"module.clip.vision_model.encoder.layers.8.self_attn.k_proj.weight\", \"module.clip.vision_model.encoder.layers.8.self_attn.k_proj.bias\", \"module.clip.vision_model.encoder.layers.8.self_attn.v_proj.weight\", \"module.clip.vision_model.encoder.layers.8.self_attn.v_proj.bias\", \"module.clip.vision_model.encoder.layers.8.self_attn.q_proj.weight\", \"module.clip.vision_model.encoder.layers.8.self_attn.q_proj.bias\", \"module.clip.vision_model.encoder.layers.8.self_attn.out_proj.weight\", \"module.clip.vision_model.encoder.layers.8.self_attn.out_proj.bias\", \"module.clip.vision_model.encoder.layers.8.layer_norm1.weight\", \"module.clip.vision_model.encoder.layers.8.layer_norm1.bias\", \"module.clip.vision_model.encoder.layers.8.mlp.fc1.weight\", \"module.clip.vision_model.encoder.layers.8.mlp.fc1.bias\", \"module.clip.vision_model.encoder.layers.8.mlp.fc2.weight\", \"module.clip.vision_model.encoder.layers.8.mlp.fc2.bias\", \"module.clip.vision_model.encoder.layers.8.layer_norm2.weight\", \"module.clip.vision_model.encoder.layers.8.layer_norm2.bias\", \"module.clip.vision_model.encoder.layers.9.self_attn.k_proj.weight\", \"module.clip.vision_model.encoder.layers.9.self_attn.k_proj.bias\", \"module.clip.vision_model.encoder.layers.9.self_attn.v_proj.weight\", \"module.clip.vision_model.encoder.layers.9.self_attn.v_proj.bias\", \"module.clip.vision_model.encoder.layers.9.self_attn.q_proj.weight\", \"module.clip.vision_model.encoder.layers.9.self_attn.q_proj.bias\", \"module.clip.vision_model.encoder.layers.9.self_attn.out_proj.weight\", \"module.clip.vision_model.encoder.layers.9.self_attn.out_proj.bias\", \"module.clip.vision_model.encoder.layers.9.layer_norm1.weight\", \"module.clip.vision_model.encoder.layers.9.layer_norm1.bias\", \"module.clip.vision_model.encoder.layers.9.mlp.fc1.weight\", \"module.clip.vision_model.encoder.layers.9.mlp.fc1.bias\", \"module.clip.vision_model.encoder.layers.9.mlp.fc2.weight\", \"module.clip.vision_model.encoder.layers.9.mlp.fc2.bias\", \"module.clip.vision_model.encoder.layers.9.layer_norm2.weight\", \"module.clip.vision_model.encoder.layers.9.layer_norm2.bias\", \"module.clip.vision_model.encoder.layers.10.self_attn.k_proj.weight\", \"module.clip.vision_model.encoder.layers.10.self_attn.k_proj.bias\", \"module.clip.vision_model.encoder.layers.10.self_attn.v_proj.weight\", \"module.clip.vision_model.encoder.layers.10.self_attn.v_proj.bias\", \"module.clip.vision_model.encoder.layers.10.self_attn.q_proj.weight\", \"module.clip.vision_model.encoder.layers.10.self_attn.q_proj.bias\", \"module.clip.vision_model.encoder.layers.10.self_attn.out_proj.weight\", \"module.clip.vision_model.encoder.layers.10.self_attn.out_proj.bias\", \"module.clip.vision_model.encoder.layers.10.layer_norm1.weight\", \"module.clip.vision_model.encoder.layers.10.layer_norm1.bias\", \"module.clip.vision_model.encoder.layers.10.mlp.fc1.weight\", \"module.clip.vision_model.encoder.layers.10.mlp.fc1.bias\", \"module.clip.vision_model.encoder.layers.10.mlp.fc2.weight\", \"module.clip.vision_model.encoder.layers.10.mlp.fc2.bias\", \"module.clip.vision_model.encoder.layers.10.layer_norm2.weight\", \"module.clip.vision_model.encoder.layers.10.layer_norm2.bias\", \"module.clip.vision_model.encoder.layers.11.self_attn.k_proj.weight\", \"module.clip.vision_model.encoder.layers.11.self_attn.k_proj.bias\", \"module.clip.vision_model.encoder.layers.11.self_attn.v_proj.weight\", \"module.clip.vision_model.encoder.layers.11.self_attn.v_proj.bias\", \"module.clip.vision_model.encoder.layers.11.self_attn.q_proj.weight\", \"module.clip.vision_model.encoder.layers.11.self_attn.q_proj.bias\", \"module.clip.vision_model.encoder.layers.11.self_attn.out_proj.weight\", \"module.clip.vision_model.encoder.layers.11.self_attn.out_proj.bias\", \"module.clip.vision_model.encoder.layers.11.layer_norm1.weight\", \"module.clip.vision_model.encoder.layers.11.layer_norm1.bias\", \"module.clip.vision_model.encoder.layers.11.mlp.fc1.weight\", \"module.clip.vision_model.encoder.layers.11.mlp.fc1.bias\", \"module.clip.vision_model.encoder.layers.11.mlp.fc2.weight\", \"module.clip.vision_model.encoder.layers.11.mlp.fc2.bias\", \"module.clip.vision_model.encoder.layers.11.layer_norm2.weight\", \"module.clip.vision_model.encoder.layers.11.layer_norm2.bias\", \"module.clip.vision_model.post_layernorm.weight\", \"module.clip.vision_model.post_layernorm.bias\", \"module.clip.visual_projection.weight\", \"module.clip.text_projection.weight\", \"module.classifier.weight\", \"module.classifier.bias\". \n",
      "Loaded 13649 captions\n",
      "Extracted 80 frames from /Users/preetham_aleti/Downloads/Explosion/Explosion050_x264A/Explosion050_x264A.mp4\n",
      "[1] CCTV footage shows a street scene with walkers, bikers, an auto, and a woman in green saree balancing flowers on her head. (0.321)\n",
      "[2] CCTV footage shows people walking and biking, with a woman in a green saree carrying flowers on her head and an auto nearby. (0.317)\n",
      "[3] Public vehicles pass roadside shops and stalls. (0.309)\n",
      "[4] Customers interacting with motorcycles as traffic flows nearby. (0.308)\n",
      "[5] Individual near vehicle with divider visible. (0.310)\n",
      "[6] person close to the vehicle appears to gesture while others stand by the shop (0.312)\n",
      "[7] man in grey is seen holding person at gunpoint while positioned close to the fuel station (0.310)\n",
      "[8] A group splits off, fighting near a parked motorbike. (0.316)\n",
      "[9] CCTV footage revealing the event. (0.317)\n",
      "[10] CCTV captures a woman in green saree screaming as two bikers snatch her chain and ride away quickly. (0.316)\n",
      "[11] A gas station is engulfed in fire with vehicles parked nearby and people rushing to safety. (0.337)\n",
      "[12] A large explosion erupts from the vehicle. (0.328)\n",
      "[13] CCTV captures a woman in a green saree walking with flowers on her head, as people move around her on foot, bikes, and an auto passes by. (0.322)\n",
      "[14] The victim tries to twist away as both attackers forcefully reach for his belongings. (0.314)\n",
      "[15] The struggle intensifies as both thieves and the shopkeeper with his wife continue fighting inside the shop. (0.311)\n",
      "[16] Smoke rises at an intersection while a person and an animal occupy the road. (0.320)\n",
      "[17] Camera shows people near the car and barriers from a different angle. (0.322)\n",
      "[18] People respond urgently to help the person on fire at the gas station. (0.318)\n",
      "[19] The dashcam continues to record the residential street in the immediate aftermath of a powerful explosion. The white pickup truck remains parked on the right side of the street. A substantial amount of smoke and debris still fills the air, though it may be showing very slight signs of further dispersal. Large and small fragments are still visible within the smoke. The houses lining the street show damage from the explosion. (0.319)\n",
      "[20] Person using mobile phone near fuel pump (0.317)\n",
      "\n",
      "================================================================================\n",
      "üé¨ VIDEO STORY: Explosion050_x264A.mp4\n",
      "================================================================================\n",
      "üïí 0.0s - 0.3s | üí¨ CCTV footage shows a street scene with walkers, bikers, an auto, and a woman in green saree balancing flowers on her head. (confidence: 0.321)\n",
      "üïí 0.4s - 0.8s | üí¨ CCTV footage shows people walking and biking, with a woman in a green saree carrying flowers on her head and an auto nearby. (confidence: 0.317)\n",
      "üïí 0.9s - 1.3s | üí¨ Public vehicles pass roadside shops and stalls. (confidence: 0.309)\n",
      "üïí 1.4s - 1.7s | üí¨ Customers interacting with motorcycles as traffic flows nearby. (confidence: 0.308)\n",
      "üïí 1.8s - 2.2s | üí¨ Individual near vehicle with divider visible. (confidence: 0.310)\n",
      "üïí 2.3s - 2.7s | üí¨ person close to the vehicle appears to gesture while others stand by the shop (confidence: 0.312)\n",
      "üïí 2.8s - 3.1s | üí¨ man in grey is seen holding person at gunpoint while positioned close to the fuel station (confidence: 0.309)\n",
      "üïí 3.2s - 3.6s | üí¨ A group splits off, fighting near a parked motorbike. (confidence: 0.316)\n",
      "üïí 3.7s - 4.0s | üí¨ CCTV footage revealing the event. (confidence: 0.317)\n",
      "üïí 4.2s - 4.5s | üí¨ CCTV captures a woman in green saree screaming as two bikers snatch her chain and ride away quickly. (confidence: 0.316)\n",
      "üïí 4.6s - 5.0s | üí¨ A gas station is engulfed in fire with vehicles parked nearby and people rushing to safety. (confidence: 0.337)\n",
      "üïí 5.1s - 5.4s | üí¨ A large explosion erupts from the vehicle. (confidence: 0.328)\n",
      "üïí 5.6s - 5.9s | üí¨ CCTV captures a woman in a green saree walking with flowers on her head, as people move around her on foot, bikes, and an auto passes by. (confidence: 0.322)\n",
      "üïí 6.0s - 6.4s | üí¨ The victim tries to twist away as both attackers forcefully reach for his belongings. (confidence: 0.314)\n",
      "üïí 6.5s - 6.8s | üí¨ The struggle intensifies as both thieves and the shopkeeper with his wife continue fighting inside the shop. (confidence: 0.311)\n",
      "üïí 6.9s - 7.3s | üí¨ Smoke rises at an intersection while a person and an animal occupy the road. (confidence: 0.320)\n",
      "üïí 7.4s - 7.8s | üí¨ Camera shows people near the car and barriers from a different angle. (confidence: 0.322)\n",
      "üïí 7.9s - 8.2s | üí¨ People respond urgently to help the person on fire at the gas station. (confidence: 0.318)\n",
      "üïí 8.3s - 8.7s | üí¨ The dashcam continues to record the residential street in the immediate aftermath of a powerful explosion. The white pickup truck remains parked on the right side of the street. A substantial amount of smoke and debris still fills the air, though it may be showing very slight signs of further dispersal. Large and small fragments are still visible within the smoke. The houses lining the street show damage from the explosion. (confidence: 0.319)\n",
      "üïí 8.8s - 9.2s | üí¨ Person using mobile phone near fuel pump (confidence: 0.317)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "üìö Summarizing captions using BART...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìå Final Video Summary:\n",
      "CCTV footage shows a street scene with walkers, bikers, an auto, and a woman in green saree balancing flowers on her head. A gas station is engulfed in fire with vehicles parked nearby and people rushing to safety. The dashcam continues to record the residential street in the immediate aftermath of a powerful explosion. A substantial amount of smoke and debris still fills the air, though it may be showing very slight signs of further dispersal. Large and small fragments are still visible within the smoke.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T15:18:33.223570Z",
     "start_time": "2025-05-28T15:18:20.719235Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import cv2\n",
    "import shutil\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "def extract_frames_from_video(video_path, frames_dir, frame_rate=1):\n",
    "    \"\"\"\n",
    "    Extract frames from video at given frame_rate (frames per second).\n",
    "    Saves extracted frames as .jpg files in frames_dir.\n",
    "    \"\"\"\n",
    "    if os.path.exists(frames_dir):\n",
    "        shutil.rmtree(frames_dir)  # Clean up old frames folder\n",
    "    os.makedirs(frames_dir)\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(f\"Cannot open video file: {video_path}\")\n",
    "\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    interval = int(fps // frame_rate) if fps > 0 else 1\n",
    "    count = 0\n",
    "    saved_count = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if count % interval == 0:\n",
    "            frame_path = os.path.join(frames_dir, f\"frame_{saved_count:05d}.jpg\")\n",
    "            cv2.imwrite(frame_path, frame)\n",
    "            saved_count += 1\n",
    "        count += 1\n",
    "\n",
    "    cap.release()\n",
    "    print(f\"Extracted {saved_count} frames to {frames_dir}\")\n",
    "\n",
    "def load_clip_model(model_path):\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Checkpoint not found: {model_path}\")\n",
    "    state_dict = torch.load(model_path, map_location=\"cpu\")\n",
    "    try:\n",
    "        model.load_state_dict(state_dict, strict=False)\n",
    "        print(\"‚úÖ Model loaded.\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to load model weights: {e}\")\n",
    "    model.eval()\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    return model, processor\n",
    "\n",
    "def classify_image(model, processor, image_path, text_labels, top_k=5):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(text=text_labels, images=image, return_tensors=\"pt\", padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    image_embeds = outputs.image_embeds / outputs.image_embeds.norm(dim=-1, keepdim=True)\n",
    "    text_embeds = outputs.text_embeds / outputs.text_embeds.norm(dim=-1, keepdim=True)\n",
    "    similarities = (image_embeds @ text_embeds.T)[0]\n",
    "    top_k_indices = similarities.topk(top_k).indices\n",
    "    return [(text_labels[i], similarities[i].item()) for i in top_k_indices]\n",
    "\n",
    "def run_inference_on_frames(frames_dir):\n",
    "    # Use your existing crime and evidence labels here (shortened for example)\n",
    "    model_path = \"clip_crime_classifier1.pt\"\n",
    "    model, processor = load_clip_model(model_path)\n",
    "\n",
    "    crime_labels = [\"robbery\", \"explosion\", \"assault\", \"kidnapping\", \"arson\", \"vandalism\", \"shooting\", \"theft\", \"fight\", \"accident\",\"normal action\"]\n",
    "    # Use your full evidence_labels list from your original code\n",
    "    evidence_labels = [\n",
    "    \"knife\", \"gun\", \"blood\", \"weapon\", \"dead\", \"lying\", \"suspicious\", \"fight\", \"attack\", \"injury\", \"explosion\", \"fire\",\n",
    "    \"pistol\", \"rifle\", \"body\", \"corpse\", \"skull\", \"molotov\", \"bullet shells\", \"syringe\", \"chains\", \"rope\", \"handcuffs\",\n",
    "    \"bat\", \"metal rod\", \"glass shard\", \"drugs\", \"cash bundle\", \"suitcase\", \"torn clothes\", \"broken phone\", \"firearm\",\n",
    "    \"blunt object\", \"axe\", \"machete\", \"hammer\", \"crowbar\", \"scalpel\", \"shiv\", \"box cutter\", \"brass knuckles\",\n",
    "    \"stun gun\", \"pepper spray\", \"garrote\", \"baton\", \"metal pipe\", \"switchblade\", \"nail gun\", \"bullet\", \"shell casing\",\n",
    "    \"spent cartridge\", \"gunpowder residue\", \"blood spatter\", \"brain matter\", \"hair strands\", \"fingerprint\", \"shoeprint\",\n",
    "    \"footprint\", \"broken glass\", \"semen stain\", \"urine sample\", \"burn marks\", \"ligature marks\", \"defensive wounds\",\n",
    "    \"duct tape\", \"zip tie\", \"gag\", \"blindfold\", \"chloroform bottle\", \"acid bottle\", \"gloves\", \"balaclava\", \"ski mask\",\n",
    "    \"face mask\", \"surgical gloves\", \"latex gloves\", \"bloody cloth\", \"wet towel\", \"gasoline can\", \"fire debris\",\n",
    "    \"ash residue\", \"charred remains\", \"scorch mark\", \"bullet hole\", \"burnt paper\", \"exploded device\", \"broken lock\",\n",
    "    \"bent door\", \"scratched surface\", \"lockpick set\", \"ID card\", \"fake passport\", \"credit card\", \"stolen phone\",\n",
    "    \"burner phone\", \"sim card\", \"CCTV footage\", \"notebook\", \"map with markings\", \"surveillance photo\", \"threat letter\",\n",
    "    \"ransom note\", \"USB drive\", \"hard disk\", \"memory card\", \"wallet\", \"driver's license\", \"personal photo\", \"watch\",\n",
    "    \"ring\", \"bloodied shoe\", \"muddy boots\", \"missing clothing\", \"ripped shirt\", \"torn dress\", \"evidence bag\",\n",
    "    \"crime scene tape\", \"silencer\", \"tripwire\", \"booby trap\", \"grenade\", \"sniper rifle\", \"projectile\", \"smoke bomb\",\n",
    "    \"carbon monoxide canister\", \"fire starter\", \"fuse wire\", \"circuit board\", \"detonator\", \"gas leak\", \"blood trail\",\n",
    "    \"drag marks\", \"vomit stain\", \"fingerprint powder\", \"DNA swab\", \"bite mark\", \"nail scrapings\", \"glass particles\",\n",
    "    \"paint chip\", \"soil sample\", \"fibers\", \"gunshot residue\", \"blood droplets\", \"taser\", \"wire cutters\", \"pliers\",\n",
    "    \"torch\", \"headlamp\", \"ladder\", \"paracord\", \"spiked object\", \"whip\", \"screwdriver\", \"socket wrench\", \"drill\",\n",
    "    \"nail\", \"saw\", \"shovel\", \"climbing gear\", \"ski cap\", \"work boots\", \"combat boots\", \"disguise\", \"makeup kit\",\n",
    "    \"fake beard\", \"wig\", \"mirror fragment\", \"red-stained clothing\", \"soaked glove\", \"burnt shoes\", \"burnt wallet\",\n",
    "    \"twine\", \"twisted metal\", \"credit card reader\", \"POS skimmer\", \"pin pad\", \"cloned card\", \"forged signature\",\n",
    "    \"bank note\", \"fake cheque\", \"contract\", \"confession note\", \"manifest\", \"registry\", \"diary\", \"blackmail note\",\n",
    "    \"spy camera\", \"drone\", \"bug detector\", \"tracking device\", \"microphone\", \"hidden recorder\", \"wiretap\",\n",
    "    \"GPS tracker\", \"browser history\", \"deleted messages\", \"burned documents\", \"printer ink\", \"USB cable\",\n",
    "    \"data cable\", \"footprint in blood\", \"smeared prints\", \"smashed camera\", \"punch hole\", \"broken furniture\",\n",
    "    \"spray paint\", \"graffiti\", \"blood pool\", \"trail of tears\", \"hysterical victim\", \"witness statement\",\n",
    "    \"torn notebook\", \"security tag\", \"alarm sensor\", \"motion detector\", \"entry log\", \"access card\", \"forged badge\",\n",
    "    \"suspicious note\", \"powdery substance\", \"chemical residue\", \"gas mask\", \"hazmat suit\", \"burner laptop\",\n",
    "    \"encrypted phone\", \"person running\", \"flames fire\",\"Explosion aftermath\",\"Rescue operation\",\"Disaster response\",\"Fire damage\",\n",
    "        \"Explosion aftermath\", \"Fire damage\", \"Scattered debris\", \"Black smoke\", \"Firefighters at scene\",\n",
    "\"Destroyed vehicles\", \"Collapsed buildings\", \"Rescue operation\", \"Emergency response units\",\n",
    "\"Injured civilians\", \"Charred remains\", \"Flash burns\", \"Smoke inhalation victims\", \"Craters on ground\",\n",
    "\"Damaged infrastructure\", \"Shattered glass\", \"Shockwave damage\", \"Burnt clothing\", \"Heat distortion\",\n",
    "\"Bomb squad presence\", \"Evacuation in progress\", \"Military cordon\", \"Hazmat suits\", \"Surveillance footage of blast\",\n",
    "\"Sound of explosion\", \"Witness reports\", \"Forensic chemical traces\", \"Explosive residue\", \"Satellite imagery of blast\",\n",
    "\"Thermal imaging\", \"Before/after photos\", \"Seismograph spike\", \"Damaged electronics\", \"Overturned vehicles\",\n",
    "\"Melted metals\", \"High-temperature indicators\", \"Blast pressure marks\", \"Security camera destruction\",\n",
    "\"Media reporting live\", \"Flash of light\", \"Sudden power outages\", \"Flying debris injuries\",\n",
    "\"Hospital surge\", \"Burn units activated\", \"Soot-covered victims\", \"Displaced people\", \"Airspace restrictions\",\n",
    "\"Shockwave-injured animals\", \"Search and rescue dogs\", \"Structural engineers onsite\",\n",
    "\n",
    "\"Physical altercation\", \"Punches thrown\", \"Visible bruises\", \"Blood stains\", \"Crowd gathering\",\n",
    "\"Police breaking up fight\", \"Verbal aggression\", \"Weapons drawn\", \"Security camera footage\", \"Bodycam recordings\",\n",
    "\"CCTV footage\", \"Broken bottles\", \"Pulled hair\", \"Torn clothing\", \"Screaming\", \"Knife wounds\",\n",
    "\"Hospital reports\", \"Eyewitness testimony\", \"Self-defense stances\", \"Bystander videos\", \"Arrest records\",\n",
    "\"911 calls\", \"Social media footage\", \"Conflicting statements\", \"Injury reports\", \"Physical evidence on ground\",\n",
    "\"Trampled area\", \"Thrown objects\", \"Police batons\", \"Tasers used\", \"Pepper spray evidence\", \"Handcuff marks\",\n",
    "\"Footage of chase\", \"Disturbed public area\", \"Fight started in queue\", \"Nightclub security footage\",\n",
    "\"Bar fight scene\", \"Street brawl\", \"Schoolyard altercation\", \"Confiscated weapons\", \"Face injuries\",\n",
    "\"Split lips\", \"Black eyes\", \"Smashed furniture\", \"Police statement\", \"Witness cell footage\", \"Brawl aftermath\",\n",
    "\"Adrenaline effects\", \"Medical treatment logs\", \"Offensive gestures\",\n",
    "\n",
    "\"Forced entry\", \"Broken locks\", \"Surveillance footage\", \"Mask-wearing suspect\", \"Glove prints\",\n",
    "\"Empty safe\", \"Stolen valuables\", \"Threats with weapon\", \"Demand notes\", \"Security alarm triggered\",\n",
    "\"Witness testimony\", \"Gunshot sound\", \"Panic button pressed\", \"Store clerks hiding\", \"Cash register emptied\",\n",
    "\"Abandoned getaway vehicle\", \"Dropped loot\", \"Forensic evidence\", \"Glove fibers\", \"Security tape review\",\n",
    "\"Bank teller account\", \"Vault tampering\", \"ATM ripped open\", \"Smashed glass\", \"Crowbar found\",\n",
    "\"CCTV malfunction\", \"Phone jammers\", \"Multiple suspects\", \"Police pursuit\", \"Police sketches\",\n",
    "\"Security guard overpowered\", \"Use of disguise\", \"Timing of heist\", \"Inside job suspicion\",\n",
    "\"Drilled locks\", \"Camera blackout\", \"Customer witness\", \"Fake IDs\", \"Unusual deposits\", \"Tool marks\",\n",
    "\"Fingerprint recovery\", \"Silent alarm logs\", \"DNA swab from scene\", \"Safe-cracking tools\", \"Entry point photos\",\n",
    "\"Surrounding business footage\", \"Abandoned evidence\", \"Loot bag found\", \"Fake license plates\",\n",
    "\"Escape route tracing\", \"Police barrier breach\",\n",
    "\n",
    "\"Unpaid items in bag\", \"Security footage\", \"No receipt\", \"Concealed merchandise\", \"Price tag removal\",\n",
    "\"Security tag tampering\", \"In-store alarms\", \"Store detective report\", \"Suspicious behavior\", \"Camera footage\",\n",
    "\"Bag check failure\", \"Hidden compartments\", \"Items under clothing\", \"Unusual customer route\", \"Acting nervously\",\n",
    "\"Rushed exit\", \"Cashier distraction\", \"Blind spots visited\", \"Multiple entry attempts\", \"Dressing room concealment\",\n",
    "\"Magnetic detacher possession\", \"Recovered stolen goods\", \"Admission of guilt\", \"Inventory mismatch\",\n",
    "\"Staff report\", \"Frequent offender\", \"Cellphone jammer\", \"Fake returns\", \"Returned stolen items\",\n",
    "\"Store layout exploitation\", \"Distraction technique\", \"Witness confrontation\", \"Unattended carts\",\n",
    "\"Companion as lookout\", \"Fake disability use\", \"Suspicious bag size\", \"Overcrowded checkout lane\",\n",
    "\"Tampered packaging\", \"Evasive responses\", \"Rapid clothing change\", \"Discarded wrappers\", \"Matching descriptions\",\n",
    "\"Unscanned items\", \"Detained by staff\", \"Portable scanner blocker\", \"Frequent item handling\",\n",
    "\"Retail chain alert\", \"Repeat pattern detection\", \"Reluctance to open bag\", \"Disguised identity\"\n",
    "] # truncated for example\n",
    "\n",
    "    frame_paths = sorted([os.path.join(frames_dir, f) for f in os.listdir(frames_dir) if f.endswith(\".jpg\")])\n",
    "    crime_votes = []\n",
    "    evidence_dict = Counter()\n",
    "\n",
    "    print(f\"üîç Processing {len(frame_paths)} frames from: {frames_dir}\")\n",
    "\n",
    "    for frame in frame_paths:\n",
    "        crime_preds = classify_image(model, processor, frame, crime_labels, top_k=1)\n",
    "        top_crime = crime_preds[0][0]\n",
    "        crime_votes.append(top_crime)\n",
    "\n",
    "        evidence_preds = classify_image(model, processor, frame, evidence_labels, top_k=3)\n",
    "        for label, _ in evidence_preds:\n",
    "            evidence_dict[label] += 1\n",
    "\n",
    "    most_common_crime = Counter(crime_votes).most_common(1)[0][0]\n",
    "    top_evidence = evidence_dict.most_common(10)\n",
    "\n",
    "    print(\"\\nüìå Final Crime Classification:\", most_common_crime.upper())\n",
    "    print(\"üßæ Top Evidence Found:\")\n",
    "    for label, count in top_evidence:\n",
    "        print(f\"  - {label} ({count} frames)\")\n",
    "\n",
    "    return {\n",
    "        \"crime_type\": most_common_crime,\n",
    "        \"top_evidence\": top_evidence\n",
    "    }\n",
    "\n",
    "def run_inference(video_path):\n",
    "    frames_dir = \"/tmp/video_frames\"  # Temporary folder for frames\n",
    "    extract_frames_from_video(video_path, frames_dir, frame_rate=1)  # 1 FPS extraction\n",
    "    return run_inference_on_frames(frames_dir)\n",
    "\n",
    "# Run inference on your video:\n",
    "summary = run_inference(\"/Users/preetham_aleti/Desktop/IMG_8679.MOV\")\n",
    "print(summary)\n"
   ],
   "id": "110bf2744589c007",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 3 frames to /tmp/video_frames\n",
      "‚úÖ Model loaded.\n",
      "üîç Processing 3 frames from: /tmp/video_frames\n",
      "\n",
      "üìå Final Crime Classification: NORMAL ACTION\n",
      "üßæ Top Evidence Found:\n",
      "  - personal photo (1 frames)\n",
      "  - burner laptop (1 frames)\n",
      "  - Portable scanner blocker (1 frames)\n",
      "  - Loot bag found (1 frames)\n",
      "  - Unscanned items (1 frames)\n",
      "  - Abandoned evidence (1 frames)\n",
      "  - Witness cell footage (1 frames)\n",
      "  - Bodycam recordings (1 frames)\n",
      "  - Security camera footage (1 frames)\n",
      "{'crime_type': 'normal action', 'top_evidence': [('personal photo', 1), ('burner laptop', 1), ('Portable scanner blocker', 1), ('Loot bag found', 1), ('Unscanned items', 1), ('Abandoned evidence', 1), ('Witness cell footage', 1), ('Bodycam recordings', 1), ('Security camera footage', 1)]}\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "20895828673da265"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
